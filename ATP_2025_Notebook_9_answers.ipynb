{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/ATP_2025_Notebook_9_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hands-on 9-1"
      ],
      "metadata": {
        "id": "3LcrxY1pbtf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-1.1\n",
        "The Github repo https://github.com/MarkHershey/CompleteTrumpTweetsArchive has all the tweets published by Donald Trump when he was at office. **Clone** the repository to your home folder on the server.\n",
        "\n",
        "```\n",
        "git clone https://github.com/MarkHershey/CompleteTrumpTweetsArchive\n",
        "```\n"
      ],
      "metadata": {
        "id": "6bHKqmAEbzSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-1.2\n",
        "Count the most frequent hashtags (#) and / or handles (@) of the dataset covering Tweets when Trump was in office. Make sure to ignore possible punctuations to avoid losing data, e.g.:\n",
        "```\n",
        "@realDonaldTrump:\n",
        "@realDonaldTrump\n",
        "```\n",
        "\n",
        "Most frequent hashtags:\n",
        "* (N.B. to get the entire tweets, use `\"` as separator; since the tweets can include commas, using the comma will give only parts of some tweets)\n",
        "```\n",
        "# !/bin/bash\n",
        "\n",
        "cat realDonaldTrump_in_office.csv | #print the file\n",
        "    cut -f 2 -d '\"'  | #extract the column with the tweets\n",
        "    tr ' ' '\\n' | #token per line\n",
        "    egrep \"^#\" | #grep the lines starting with a hashtag\n",
        "    perl -pe 's/[[:punct:]]$//g' | #remove punctuation in the end of line\n",
        "    egrep -v \"^$\" | #remove empty lines\n",
        "    sort | #frequency list\n",
        "    uniq -c |\n",
        "    sort -nr\n",
        "```\n",
        "Run like this:\n",
        "* N.B. `less` in the end of a pipe is useful as it gives the output in an easy-to-read and searchable format.\n",
        "```\n",
        "./your_script_hashtags.sh | less`\n",
        "```\n"
      ],
      "metadata": {
        "id": "SlDU-K7Bb1lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-1.3\n",
        "Make a script that takes a tweet handle as an argument and prints out its distribution over time month by month. Run the script on a couple of interesting handles / hashtags. Do you see any trends?\n",
        "\n",
        "```\n",
        "# !/bin/bash\n",
        "\n",
        "# run: cat file.csv | ./your_script.sh handle\n",
        "\n",
        "# frequency list of timestamps\n",
        "\n",
        "egrep -i $1 | # grep for lines with the handle / hashtag\n",
        "    cut -f 2 -d ',' | # take the 2nd column (the timestamps)\n",
        "    cut -f 2 -d ' ' | # take the date; N.B. the date is in the 2nd column although there is no visible 1st column\n",
        "    cut -f 1,2 -d '-'| # take just the years and months\n",
        "    sort |\n",
        "    uniq -c |\n",
        "    sort -rn #count frequencies\n",
        "```\n",
        "\n",
        "**Extra:** sort the output so that you have the tweets ordered from older to newer, followed by the number of tweets for that time stamp, like this:\n",
        "\n",
        "```\n",
        "YEAR-MONTH1 NUM-OF-TWEETS\n",
        "YEAR-MONTH2 NUM-OF-TWEETS\n",
        "```\n",
        "If you get a permission denied error, you have forgotten to add execution rights to your script. This can be done with `chmod a+rwx file.txt`\n",
        "\n",
        "So what to do:\n",
        "* switch columns the other way around with Perl, regex capture groups, and back referencing\n",
        "* sort by the 2nd column (timestamp) in ascending order\n",
        "\n",
        "```\n",
        "perl -pe 's/^ *([0-9]+) ([0-9-]+)/$2 $1/g' |\n",
        "  sort -n\n",
        "```\n"
      ],
      "metadata": {
        "id": "8R1wAlVXb7Bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 9-1.4\n",
        "Make another script that takes a handle as an argument and prints out a cleaned and normalized frequency list of the words that occur in the tweets with the handle.\n",
        "\n",
        "You can try out different ways of cleaning the data. Does it make sense to include tokens with numbers and / or punctuation at all? Or is it better to just, e.g., delete tokens and numbers and otherwise keep the strings there?\n",
        "\n",
        "```\n",
        "# !/bin/bash\n",
        "\n",
        "# run: cat your_file.txt | ./your_script.sh handle | less\n",
        "\n",
        "# frequency list of tweets with a specific handle/hashtag\n",
        "\n",
        "cut -f 2 -d '\"' | #set the separator to \" in order to get the entire text in the tweets\n",
        "    egrep -i $1 | #grep lines with the handle given as an argument when you run the script\n",
        "    #tr ' ' '\\n' | #word per line; either tr or perl\n",
        "    perl -pe 's/ /\\n/g' |\n",
        "    egrep -v '^[[:punct:]]|[0-9]' | #remove puncts and numbers\n",
        "    tr '[[:upper:]]' '[[:lower:]]' | #normalize text\n",
        "    egrep -v \"^$\" | #remove empty lines\n",
        "    sort | #frequency list\n",
        "    uniq -c |\n",
        "    sort -nr\n",
        "```"
      ],
      "metadata": {
        "id": "j7OBp1ykX3nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on 9-2\n",
        "\n",
        "Finnish parliamentary speeches have been published in parlamenttisampo.fi but it is difficult to get large amounts of data from there. Luckily the full datasets are available as yearly CSV files here:\n",
        "https://a3s.fi/parliamentsampo/speeches/csv/index.html\n",
        "\n",
        "Unlucky for us, the format of the file is not optimal for our tools, but let's see what we can get out of it anyway."
      ],
      "metadata": {
        "id": "LQ9ydqD5cKSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-2.1\n",
        "\n",
        "Get the speech file for year 2020. Browse the file to see what it contains. Notice how it is comma separated and how the actual speech (\"content\") is separated by double quotes. More advanced tools could handle that, but out cut tool cannot.\n",
        "\n",
        "(N.B. A good way to deal with this would be to use e.g. Python and the Pandas library, but that is advanced stuff. We will make do with more simiple tools.)\n",
        "\n",
        "Make a pipe that only uses the first line of text (containing the headers) and prints a list of headers, one per line.\n",
        "\n",
        "```\n",
        "wget https://a3s.fi/parliamentsampo/speeches/csv/speeches_2020.csv\n",
        "less speeches_2020.csv  # exit with q\n",
        "\n",
        "head -1 speeches_2020.csv | tr ',' '\\n'\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "azxf3z690ZqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-2.2\n",
        "\n",
        "Find out the 10 last names (column \"family\") that have given the most speeches in 2020.\n",
        "\n",
        "Find out the 10 full names (column \"name_in_source\") that have given the most speeches in 2020.\n",
        "\n",
        "```\n",
        "cat speeches_2020.csv | cut -f7 -d ',' | sort | uniq -c | sort -rn | head\n",
        "\n",
        "cat speeches_2020.csv | cut -f23 -d ',' | sort | uniq -c | sort -rn | head\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "6pMxz9QQ0kmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 9-2.3\n",
        "\n",
        "\n",
        "Find out which months were the most active (most speeches given) and which months were the least active (least speeches given) for debate.\n",
        "\n",
        "Which months are missing? Why do you think this is?\n",
        "\n",
        "```\n",
        "cat speeches_2020.csv |\n",
        "  cut -f3 -d ',' |\n",
        "  cut -f1,2 -d '-' |\n",
        "  egrep '^2020' | # to get rid of noise\n",
        "  sort |\n",
        "  uniq -c |\n",
        "  sort -rn |\n",
        "  head -20  # to make sure all 12 months are shown and to see if something else was caught\n",
        "```\n"
      ],
      "metadata": {
        "id": "XG9H19D50mWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-2.4\n",
        "\n",
        "Try to find out the most dicussed topic.\n",
        "\n",
        "```\n",
        "cat speeches_2020.csv | cut -f10 -d ',' | sort | uniq -c | sort -rn | head -20\n",
        "\n",
        "# this does not really give us anything useful :/\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "eNsch80e8kzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-2.5\n",
        "\n",
        "Try to find out the party that spoke the most often.\n",
        "\n",
        "\n",
        "```\n",
        "cat speeches_2020.csv | cut -f9 -d ',' | sort | uniq -c | sort -rn | head -20\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8x1wwn4DAJEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4vS5s4vzB2zo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on 9-3"
      ],
      "metadata": {
        "id": "VUXm2FYJ0Rsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 9-3.1\n",
        "\n",
        "a) Create a file that contains this string of text on one line:\n",
        "```\n",
        "These are the rules:BREAKJava is trashBREAKPython is coolBREAKAnd so is Bash\n",
        "```\n",
        "\n",
        "* either use nano and copypaste...\n",
        "* ...or use `echo` and save to file from stdout:\n",
        "```\n",
        "echo \"These are the rules:BREAKJava is trashBREAKPython is coolBREAKAnd so is Bash\" > poetry.txt\n",
        "```\n",
        "\n",
        "b) Formulate a simple pipeline that reads the file you just created, uses the `perl` substitution command to change all `BREAK` words into newlines `\\n`, then egreps all lines that have the word \"is\"\n",
        "\n",
        "```\n",
        "cat poetry.txt | perl -pe 's/BREAK/\\n/g' | egrep \"is\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JtyNJqT9cB7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-3.2\n",
        "\n",
        "a) Make a script file called `print_rules.sh` that contains the previous pipeline but divided on multiple lines for readability (remember to indent!).\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "cat poetry.txt |\n",
        "  perl -pe 's/BREAK/\\n/g' |\n",
        "  egrep \"is\"\n",
        "```\n",
        "\n",
        "b) Then add one command to the beginning of the script that prints the text string \"RULES:\", then add another command to the end of the script that prints (`Anna`).\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "echo \"RULES:\"\n",
        "cat poetry.txt |\n",
        "  perl -pe 's/BREAK/\\n/g' |\n",
        "  egrep \"is\"\n",
        "echo \"Anna\"\n",
        "```\n",
        "\n",
        "c) Modify the last line so that you can give any name as an argument (replace \"Anna\" with something). Test the script with your name.\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "echo \"RULES:\"\n",
        "cat poetry.txt |\n",
        "  perl -pe 's/BREAK/\\n/g' |\n",
        "  egrep \"is\"\n",
        "echo $1\n",
        "```\n",
        "\n",
        "Run with:\n",
        "```\n",
        "./print_rules.sh somenamehere\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "P0-B08zicCXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9-3.3\n",
        "\n",
        "Modify the Perl script so that it finds all instances of \"is \" that are followed by a lowercase character and substitutes them with \"is super \", like this:\n",
        "```\n",
        "Python is cool >>>> Python is super cool\n",
        "And so is Bash >>>> And so is Bash\n",
        "```\n",
        "\n",
        "Use regex capture groups and back reference!!\n",
        "\n",
        "```\n",
        "# !/bin/bash\n",
        "echo \"RULES:\"\n",
        "cat poetry.txt |\n",
        "  perl -pe 's/BREAK/\\n/g' |\n",
        "  perl -pe 's/(is) ([[:lower:]])/$1 super $2/g' |\n",
        "  egrep \"is\"\n",
        "echo $1\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hnTRHs1EcBqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hands-on 9-4\n",
        "\n",
        "Get a copy of a recipe dataset from here:\n",
        "\n",
        "`/home/ankrris/data/recipes_modified.csv`\n",
        "\n",
        "a) Find out how many recipes do not use tomatoes in the \"ingredients\" column.\n",
        "\n",
        "```\n",
        "cat recipes_modified.csv | cut -f6 -d '|' | egrep -v \"tomato\" | wc -l\n",
        "```\n",
        "\n",
        "b) Replace every \"potato\" with \"tomato\". Now how many recipes do not use any tomatoes in the \"ingredients\" column?\n",
        "\n",
        "```\n",
        "cat recipes_modified.csv | cut -f6 -d '|' | perl -pe 's/potato/tomato/g' | egrep -v \"tomato\" | wc -l\n",
        "```\n",
        "\n",
        "d) Replace every \"tomato\" and \"potato\" with \"pomato\" in a single Perl substitution. Count lines with \"pomato\".\n",
        "\n",
        "```\n",
        "cat recipes_modified.csv | cut -f6 -d '|' | perl -pe 's/(potato|tomato)/pomato/g' | egrep -v \"tomato\" | wc -l\n",
        "```\n",
        "\n",
        "e) Make a script that substitutes every word that begins with an uppercase letter with that same word plus \"-ho\". E.g. \"Place\" >>>>> \"Place-ho\". Provide the file to the script as an argument.\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "cat $1 | perl -pe 's/\\b([A-Z][a-z]*)\\b/$1-ho/g'\n",
        "```\n",
        "\n",
        "* \\b — word boundary (ensures we match whole words).\n",
        "\n",
        "* [A-Z] — first character must be uppercase.\n",
        "\n",
        "* [a-z]* — zero or more lowercase letters after that.\n",
        "\n",
        "* $1-ho — puts back the captured word and adds -ho."
      ],
      "metadata": {
        "id": "Ln_46oEkQbCg"
      }
    }
  ]
}