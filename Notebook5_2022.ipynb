{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/Notebook5_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        "\n",
        "* Let's use the CORE-corpus availablet at https://github.com/TurkuNLP/CORE-corpus. Clone it to your notebook. Focus on the development set of the dataset. How many documents does it include?\n",
        "\n",
        "* Column 1 indicates the register abbreviations of the documents. They are explained in the txt-file found in the repo. What are the 10 most frequent register combinations found in the dataset? What actual classes do these abbreviations correspond to?\n",
        "\n",
        "* Select two registers you want to focus on. How to make a regular expression that matches the register label in the first column but not the actual text?  Make files that consist of frequency lists of the words of these two registers. Normalize the words first to lower case and delete numbers and punctuation. Can you think of how to combine the delete commands to one using regexes?\n",
        "\n",
        "* How many unique words do the two registers have?\n",
        "\n",
        "* How would you search for words that occur at least 100 times using regex? Note that the frequency list has sometimes whitespaces in the beginning before the numbers. See how fast the frequency of the words drops!\n",
        "\n",
        "* Have a look at the most frequent words of the registers - again, they are super-similar! To have a better idea of the differences of the two files and the texts they represent, we can use a python script that will count the keywords of the files. These are words that are statistically overrepresented in the studied file / register in comparison with another one. \n",
        "\n",
        "The keyword method is described in the following paper: \n",
        "```\n",
        "Incorporating text dispersion into keyword analyses\n",
        "Jesse Egbert and Doug Biber\n",
        "Corpora 2019 14:1, 77-104\n",
        "```\n",
        "\n",
        "* You can find a script for extracting keywords in the `scripts` folder of the course Github repo. So by cloning the repo, you will also get the script. You can run the script by\n",
        "\n",
        "```\n",
        "python3 ATP_kurssi/scripts/text_dispersion_text.py file1 file2 \n",
        "```\n",
        "\n",
        "And it will give you the keywords of the first file. Do these make sense? If you have time, you can also extract two other register classes and see how their keywords look like.\n",
        " \n",
        "### Extra\n",
        "If you have time, try to modify the data so that instead of words you have word two-grams (combinations of two words such as _two-words_ _words-such _such-as_) and then run key two-grams instead of keywords. How do those look like?"
      ],
      "metadata": {
        "id": "_hV8Po1VAmCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "\n",
        "Let's work on some Tweets. Unfortunately these are in Finnish, but use the file at http://dl.turkunlp.org/atp/all_tweets_one_line.txt.gz\n",
        "\n",
        "Basic stats\n",
        "* How many tweets?\n",
        "* Can you figure out what the tweets are about?\n",
        "* When have they been Tweeted?\n",
        "* Can you make a distribution of how many tweets per month the data has? The `cut` command can be useful for this. `-d ` can specify the delimiter, so e.g.,  `cut -f 1,2 -d ' '` will get the first and second columns separated by a whitespace.\n",
        "\n",
        "Contents\n",
        "* Let's count the most frequent words of the tweets \n",
        "* And also the most important hashtags. And user names.\n",
        "These should tell us what we have in the data!\n",
        "\n",
        "Keywords by time and / or person?\n",
        "\n",
        "* Let's count username or time-related keywords with the python above script. How do the keywords look like, can you trace anything interesting?\n"
      ],
      "metadata": {
        "id": "RUH-e4nF_1az"
      }
    }
  ]
}
