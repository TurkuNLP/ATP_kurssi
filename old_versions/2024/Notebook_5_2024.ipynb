{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/Notebook_5_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap\n",
        "\n",
        "### List of new commands from notebook 4:\n",
        "\n",
        "`git clone`, `gzip`, `zcat`, `tr`\n",
        "\n",
        "### List of regular expressions we have used:\n",
        "\n",
        "`[:punct:]`, `[0-9]`, `[:upper:]`, `[:lower:]`\n",
        "\n",
        "`^`, `$`, `|` , `[]`, `()`,\n",
        "\n",
        "### Not a regular expression but important nonetheless:\n",
        "`\\`\n",
        "\n",
        "About quotation marks: both double quotes (\") or single quotes (') work the same most of the time. If you need to use escapes (\\\\) or variables within strings, you need to think and/or test which ones produce the behaviour you want."
      ],
      "metadata": {
        "id": "ZVYm8B04O0vU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on work\n",
        "\n",
        "### Exercise 1\n",
        "\n",
        "* I (Anna) work with Finnish parliamentary speeches. Let's try to wrangle a file containing data from year 2019. NB! This file has been pre-cleaned a little to make it work here!\n",
        "\n",
        "* We will be examining the file which contains all speeches given in year 2019: https://raw.githubusercontent.com/aristila/totally-random-stuff/refs/heads/main/finparlspeeches_2019.csv\n",
        "\n",
        "\n",
        "Your tasks:\n",
        "\n",
        "1. Get the file.\n",
        "2. Check what the contents look like.\n",
        "3. One row has one speech and related metadata. How many speeches have been given this year?\n",
        "4. Some interesting columns are \"date\" (the day the speech was given), \"party\" (parliamentary faction), \"lang\" (language the speech is given in; either Finnish, Swedish, or both), and \"content\" which contains the actual speech. These columns are numbers **4, 6, 7, and 9.** Cut these four columns, (Hint: If you don't know how to cut multiple columns at once, Google it!), then save the truncated dataset as a new file called \"**speeches_2019_4c.csv**\". Use this file from now on.\n",
        "5. How many unique parties (parliamentary factions) appear in the data?\n",
        "6. Which party has given the most speeches?\n",
        "7. How many of the speeches are given only in Swedish? How many percentages is that?\n",
        "8. How many times does the word \"virus\" appear in the speeches? What about \"korona\" or \"covid\"?\n",
        "9. Get all the speeches that mention \"ilmastonmuutos\" (climate change). Which party has the most of those?\n",
        "10. On which date were there most speeches about \"ilmastonmuutos\"?\n",
        "\n",
        "EXTRA: Formulate a question of your own! What would you like to find out?"
      ],
      "metadata": {
        "id": "_hV8Po1VAmCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get the file."
      ],
      "metadata": {
        "id": "AesWyi4scGNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Check what the contents look like."
      ],
      "metadata": {
        "id": "QLHnMlgbcGLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. One row has one speech and related metadata. How many speeches have been given this year?"
      ],
      "metadata": {
        "id": "jNqRgs4IcLf7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Some interesting columns are \"date\" (the day the speech was given), \"party\" (parliamentary faction),\n",
        "# \"lang\" (language the speech is given in; either Finnish, Swedish, or both), and \"content\" which contains\n",
        "# the actual speech. These columns are numbers 4, 6, 7, and 9. Cut these four columns, (Hint: If you don't\n",
        "# know how to cut multiple columns at once, Google it!), then save the truncated dataset as a new file called\n",
        "# \"speeches_2019_4c.csv\". Use this file from now on.\n",
        "\n",
        "# NB! it says 'csv' but the separator is a pipe character (|), define it for cut with -d '|'\n",
        "# (you can Google an exampmle of this as well)"
      ],
      "metadata": {
        "id": "RFsvuoYpcLc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. How many unique parties (parliamentary factions) appear in the data?"
      ],
      "metadata": {
        "id": "q9TMqy3gcLak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Which party has given the most speeches?"
      ],
      "metadata": {
        "id": "JQcpUA6Jca_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. How many of the speeches are given only in Swedish?\n",
        "# How many percentages is that? (Google if there is a way to make Bash do calculations with decimals ;) )"
      ],
      "metadata": {
        "id": "pZYhkwDEca46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. How many times does the word \"virus\" appear in the speeches? What about \"korona\" or \"covid\"?"
      ],
      "metadata": {
        "id": "xmZUmGMTca1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Get all the speeches that mention \"ilmastonmuutos\" (climate change). Which party has the most of those?"
      ],
      "metadata": {
        "id": "QBOV5GmMcag9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. On which date were there most speeches about \"ilmastonmuutos\"?"
      ],
      "metadata": {
        "id": "boPhp0xMcaeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (EXTRA) Formulate a question of your own! What would you like to find out?"
      ],
      "metadata": {
        "id": "9TH6dv70cp8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exercise 2\n",
        "\n",
        "11. Let's use the CORE-corpus availablet at https://github.com/TurkuNLP/CORE-corpus. Clone it to your notebook. Focus on the development set of the dataset. How many documents does it include?\n",
        "\n",
        "12. Column 1 indicates the register abbreviations of the documents. They are explained in the txt-file found in the repo. What are the 10 most frequent register combinations found in the dataset? What actual classes do these abbreviations correspond to?\n",
        "\n",
        "13. Select two registers you want to focus on. How to make a regular expression that matches the register label in the first column but not the actual text?  Make files that consist of frequency lists of the words of these two registers. Normalize the words first to lower case and delete numbers and punctuation. Can you think of how to combine the delete commands to one using regexes?\n",
        "\n",
        "14. How many unique words do the two registers have?\n",
        "\n",
        "15. How would you search for words that occur at least 100 times using regex? Note that the frequency list has sometimes whitespaces in the beginning before the numbers. See how fast the frequency of the words drops!"
      ],
      "metadata": {
        "id": "JCDCa0EVa0t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Clone the repo. (Hint: Go to the website and get the repo address from the green button)"
      ],
      "metadata": {
        "id": "_mP90xr3dnNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (11. continued) Change to the right directory"
      ],
      "metadata": {
        "id": "oqpd46JciR5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (11. continued)\n",
        "# Focus on the development set of the dataset (dev.tsv.gz). How many documents does it include?"
      ],
      "metadata": {
        "id": "EflIQ8vRdnEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Column 1 indicates the register abbreviations of the documents.\n",
        "# They are explained in the txt-file found in the repo (register_label_abbreviations.txt).\n",
        "# What are the 10 most frequent register combinations found in the dataset?\n",
        "# What actual classes do these abbreviations correspond to?"
      ],
      "metadata": {
        "id": "Xw3ktbx5dm9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Select two registers you want to focus on.\n",
        "# How to make a regular expression that matches the register label in the first column but not the actual text?\n",
        "# Make new files that consist of frequency lists of the words of these two registers.\n",
        "# Normalize the words first to lower case and delete numbers and punctuation."
      ],
      "metadata": {
        "id": "_8yEI3YMdm1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. How many unique words do the two registers have?"
      ],
      "metadata": {
        "id": "VQkOhyx1dms2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. How would you search for words that occur at least 100 times using regex?\n",
        "# Note that the frequency list has sometimes whitespaces in the beginning before the numbers.\n",
        "# See how fast the frequency of the words drops! (Hint: You can google Zipf's law in NLP)"
      ],
      "metadata": {
        "id": "NlriLMKodmlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 (if there is still time) (not checked by Anna ;) )\n",
        "\n",
        "Let's work on some Tweets. Unfortunately these are in Finnish, but use the file at http://dl.turkunlp.org/atp/all_tweets_one_line.txt.gz\n",
        "\n",
        "Basic stats\n",
        "* How many tweets?\n",
        "* Can you figure out what the tweets are about?\n",
        "* When have they been Tweeted?\n",
        "* Can you make a distribution of how many tweets per month the data has? The `cut` command can be useful for this. `-d ` can specify the delimiter, so e.g.,  `cut -f 1,2 -d ' '` will get the first and second columns separated by a whitespace.\n",
        "\n",
        "Contents\n",
        "* Let's count the most frequent words of the tweets\n",
        "* And also the most important hashtags. And user names.\n",
        "These should tell us what we have in the data!\n",
        "\n",
        "Keywords by time and / or person?\n",
        "\n",
        "* Let's count username or time-related keywords with the python above script. How do the keywords look like, can you trace anything interesting?\n"
      ],
      "metadata": {
        "id": "RUH-e4nF_1az"
      }
    }
  ]
}
