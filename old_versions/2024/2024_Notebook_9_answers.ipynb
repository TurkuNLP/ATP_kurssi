{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2YaAm2zJ0jGn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/2024_Notebook_9_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please use the server to do these exercises. If you are unfamiliar with scripts, you can also start with the time-out exercises listed on Notebook8**\n",
        "\n",
        "## Exercise 1\n",
        "\n",
        "The Github repo https://github.com/MarkHershey/CompleteTrumpTweetsArchive has all the tweets published by Donald Trump when he was at office. Clone the repo to your home folder on the server."
      ],
      "metadata": {
        "id": "oS59_fLadhdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1\n",
        "Count the most frequent hashtags and / or handles of the dataset covering Tweets when Trump was in office. Make sure to ignore possible punctuations, such as\n",
        "```\n",
        "@realDonaldTrump:\n",
        "@realDonaldTrump\n",
        "```\n",
        "\n",
        "most frequent hashtags:\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "cat realDonaldTrump_in_office.csv | #print the file\n",
        "    cut -f 2 -d '\"'  | #extract the column with the tweets (N.B. to get the entire tweets, use \" as separator; the comma will give only parts of the tweets including commas)\n",
        "    tr ' ' '\\n' | #token per line\n",
        "    egrep \"^#\" | #grep the lines starting with a hashtag\n",
        "    perl -pe 's/[[:punct:]]$//g' | #remove punctuation in the end of line\n",
        "    egrep -v \"^$\" | #remove empty lines\n",
        "    sort | #frequency list\n",
        "    uniq -c |\n",
        "    sort -nr |\n",
        "    less\n",
        "```\n",
        "\n",
        "run as follows:\n",
        "\n",
        "```\n",
        "./your_script_hashtags.sh | less\n",
        "```\n",
        "\n",
        "N.B. `less` in the end of a pipe is useful as it gives the output in an easy-to-read and searchable format.\n",
        "\n",
        "most frequent handles (essentially the same as for hashtags):\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "cat realDonaldTrump_in_office.csv | #print the file\n",
        "    cut -f 2 -d '\"'  | #extract the tweets\n",
        "    #tr ' ' '\\n' | #token per line; you can use either tr or perl\n",
        "    perl -pe 's/ /\\n/g' | #token per line\n",
        "    egrep \"^@\" | #grep the lines that start with @\n",
        "    perl -pe 's/[[:punct:]]$//g'| #remove punctuation in the end of line\n",
        "    egrep -v \"^$\" | #remove empty lines\n",
        "    sort | #frequency list\n",
        "    uniq -c |\n",
        "    sort\n",
        "```\n",
        "run as:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "./your_script_handles.sh | less\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8ei-q60zdj9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2\n",
        "Make a script that takes as argument a handle and prints out its distribution over time month by month. Run the script on a couple of interesting handles / hashtags. Do you see any trends?\n",
        "\n",
        "**Extra:** sort the output so that you have the tweets ordered from older to newer, followed by the number of tweets for that time stamp, like\n",
        "\n",
        "```\n",
        "YEAR-MONTH1 NUM-OF-TWEETS\n",
        "YEAR-MONTH2 NUM-OF-TWEETS\n",
        "```\n",
        "If you get a permission denied error, you have forgotten to add execution rights to your script. This can be done with `chmod a+rwx file.txt`\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "#run: cat file.csv | ./your_script.sh handle\n",
        "\n",
        "#frequency list of timestamps\n",
        "\n",
        "egrep -i $1 | # grep for lines with the handle / hashtag\n",
        "    cut -f 2 -d ',' | # take the 2nd column (the timestamps)\n",
        "    cut -f 2 -d ' ' | # take the date; N.B. the date is in the 2nd column although there is no visible 1st column\n",
        "    cut -f 1,2 -d '-'| # take just the years and months\n",
        "    sort |\n",
        "    uniq -c |\n",
        "    sort -rn #count frequencies\n",
        "\n",
        "```\n",
        "for the extra part of the task, add the following in a pipe:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        " ###extra\n",
        "    perl -pe 's/^ *([0-9]+) ([0-9-]+)/$2 $1/g' | #switch the columns the other way around\n",
        "    sort -n # sort by the 2nd column (timestamp) in ascending order\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wwDg6GLsgG_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3\n",
        "Make another script that takes as argument a handle and prints out a cleaned and normalized frequency list of the words that occur in the tweets with the handle.\n",
        "\n",
        "You can try out different ways of cleaning the data. Does it make sense to include tokens with numbers and / or punctuation at all? Or is it better to just, e.g., delete tokens and numbers and otherwise otherwise keep the strings there?\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "#run: cat your_file.txt | ./your_script.sh handle | less\n",
        "\n",
        "#frequency list of tweets with a specific handle/hashtag\n",
        "\n",
        "cut -f 2 -d '\"' | #set the separator to \" in order to get the entire text in the tweets\n",
        "    egrep -i $1 | #grep lines with the handle given as an argument when you run the script\n",
        "    #tr ' ' '\\n' | #word per line; either tr or perl\n",
        "    perl -pe 's/ /\\n/g' |\n",
        "    egrep -v '^[[:punct:]]|[0-9]' | #remove puncts and numbers\n",
        "    tr '[[:upper:]]' '[[:lower:]]' | #normalize text\n",
        "    egrep -v \"^$\" | #remove empty lines\n",
        "    sort | #frequency list\n",
        "    uniq -c |\n",
        "    sort -nr\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qlvHK4hMhJ6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Advanced\n",
        "\n",
        "The output of 1.3 would be much better if it excluded function words, don't you think?\n",
        "\n",
        "Trankit is super slow and cannot be run on the server. But UdPipe can - the performance is lower, but maybe it would be enough. Let's try!\n",
        "\n",
        "UdPipe documentation can be found at https://lindat.mff.cuni.cz/services/udpipe/api-reference.php\n",
        "\n",
        "With the following call, UdPipe prints to standard output the conllu analysis of the input file, here called delme.csv\n",
        "\n",
        "`curl -F data=@delme.csv -F model=english -F tokenizer= -F tagger= -F parser= http://lindat.mff.cuni.cz/services/udpipe/api/process | PYTHONIOENCODING=utf-8 python -c \"import sys,json; sys.stdout.write(json.load(sys.stdin)['result'])\" `\n",
        "\n",
        "If you have time, try to implement a script that is similar to 1.3 but outputs only content POS classes, such as nouns, verbs, etc, instead of all most frequent words, and lemmas instead of running words.\n",
        "\n",
        "You can try with different POS classes to figure out the most useful output in your opinion. What happens if you include only nouns? or proper nouns? or adjetives? or all these classes?\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "#run cat your_file.csv | ./your_script.sh handle\n",
        "\n",
        "egrep -i $1 | cut -f 4 -d ',' > toparse.csv #extract the tweets, direct to a new file\n",
        "\n",
        "curl -F data=@toparse.csv -F model=english -F tokenizer= -F tagger= -F parser= http://lindat.mff.cuni.cz/services/udpipe/api/process | PYTHONIOENCODING=utf-\\\n",
        "8 python -c \"import sys,json; sys.stdout.write(json.load(sys.stdin)['result'])\" | #parse\n",
        "\n",
        "    egrep -w \"NOUN|PROPN|ADJ\" | #grep the lines with these words as individual words, i.e., not part of a word\n",
        "    cut -f 3 | #lemmas\n",
        "    tr '[:upper:]' '[:lower:]' | #normalize\n",
        "    egrep -v \"rt|&am\" | #remove\n",
        "    sort | #frequency list\n",
        "    uniq -c |\n",
        "    sort -nr |\n",
        "    less\n",
        "```"
      ],
      "metadata": {
        "id": "2YaAm2zJ0jGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tips\n",
        "A new perl script can be useful for modifying strings. This example would replace all instances of REPLACED with ORIGINAL.\n",
        "\n",
        "```\n",
        "cat inputfile.txt | perl -pe 's/ORIGINAL/REPLACED/g'\n",
        "```\n",
        "\n",
        "This script supports also back reference, so you can refer to expressions in `ORIGINAL` and refer to those with `$` in `REPLACED`. Like here any punctuation is replaced by a newline and that punctuation\n",
        "\n",
        "```\n",
        " perl -pe 's/([[:punct:]])/\\n$1/g'\n",
        "```\n",
        "----------------\n",
        "\n",
        "Note that in a script, you can have various commands, and all the commands will be executed one by one (with the exception of pipes, where the commands are executed all at once).\n",
        "\n",
        "For instance if a script has the two commands below, it will first print \"kukkuu\" to a file called delme.txt, and then, undepending of the first one, print the file and egrep lines correspomding to \"kuu\".\n",
        "\n",
        "```\n",
        "echo \"kukkuu\" > delme.txt\n",
        "\n",
        "cat delme.txt | egrep \"kuu\"\n",
        "```"
      ],
      "metadata": {
        "id": "vO_1acpfih8P"
      }
    }
  ]
}
