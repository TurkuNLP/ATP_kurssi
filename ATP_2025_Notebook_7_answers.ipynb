{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/ATP_2025_Notebook_7_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise I\n",
        "\n",
        "In the Github repo https://github.com/TurkuNLP/ATP_kurssi.git there are syntax analyzed tweets in a folder called 'data'. The file is called simply `covidtweets.conllu.gz `. Let's work on this a bit.\n",
        "\n",
        "### 1. Preparations\n",
        "\n",
        "\n",
        "*   a. clone the repo\n",
        "*   b. go to the folder\n",
        "\n",
        "\n",
        "### 2. Basics\n",
        "* a. How many tweets?\n",
        "* b. How many tokens? What if you exclude punctuation and numbers?\n",
        "* c. How many sentences? Note that `\\t` (tab which separates the columns in the file) does not work with egrep, can you google for how to do this?\n",
        "\n",
        "### 3. Lexical characteristics\n",
        "* a. The most frequent lemmas?\n",
        "* b. What if you exclude function words? The definition of function words can vary a bit. What do you think could be the most useful POS classes to keep to get a general view to the contents of the tweets?\n",
        "* Note: it might be hard to figure out the POS tags associated with the words, you can also analyze this by combining the lemma and POS columns\n",
        "\n",
        "### 4. More\n",
        "* Now that we have POS classes, we can also focus on specific kinds of words. So let's count the most frequent lemmas for\n",
        "  * a. nouns (NOUN)\n",
        "  * b. adjectives (ADJ)\n",
        "  * c. verbs (VERB)\n",
        "* Which POS class words provide the most interesting results in your opinion?"
      ],
      "metadata": {
        "id": "xx16JAjMekGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1a. Let's start by cloning the repo\n",
        "\n",
        "! git clone https://github.com/TurkuNLP/ATP_kurssi.git"
      ],
      "metadata": {
        "id": "0gnhi-wzxyoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1b. Then go to the folder where the covidtweets file is\n",
        "\n",
        "%cd ATP_kurssi/data"
      ],
      "metadata": {
        "id": "IvI-K5jzeEJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "9g0IXcN_5T6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#As always, let's start by checking the file\n",
        "\n",
        "#remember to print the gzipped file with zcat\n",
        "\n",
        "! zcat covidtweets.conllu.gz| head -20 # we can see that each tweet starts with the mention ###C: NEWDOC\n"
      ],
      "metadata": {
        "id": "V85_1LiJeI-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2a. Count the tweets\n",
        "# we can just grep for ###C: NEWDOC indicating the start of a new document and count the lines\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^###C: NEWDOC\" | wc -l"
      ],
      "metadata": {
        "id": "oSQ_h_ZphIEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2b. Count the tokens\n",
        "# for tokens, we need to focus on the lines starting with a number and count those\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | head #these seem to match the correct lines"
      ],
      "metadata": {
        "id": "D7UwWbDBhS2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the lines for token count\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | wc -l"
      ],
      "metadata": {
        "id": "fvgQ5Fv_hcRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to exclude tokens tagged as numbers or punctuation, we should exclude lines with those tags\n",
        "# first we need to figure out those tags.\n",
        "# this can be searched for too!\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"!\" | head"
      ],
      "metadata": {
        "id": "Re8aeKZ2h13C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N.B.** The fourth column gives the POS tag for a token, and here we see that punctuation marks are tagged with PUNCT. We can then `egrep -v \"PUNCT\"` to exclude punctuation."
      ],
      "metadata": {
        "id": "dmVtkPGJW8gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#similarly, we can find the POS tag for numbers\n",
        "#to make it a bit simpler (and easier to read), I'll keep the columns 2 (for the running words) and 4 (POS)\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | cut -f 2,4 | egrep \"[0-9]\" | head"
      ],
      "metadata": {
        "id": "qV_Bh-3kiMId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that the POS tag for numbers is NUM. We can `egrep -v \"NUM|PUNCT\"` to remove both numbers and punctuation simultaneously.\n",
        "\n",
        "Remember that `|`is alternative, and thus `egrep -v \"NUM|PUNCT\"`matches lines that don't include NUM or PUNCT."
      ],
      "metadata": {
        "id": "YguCD_8OXrqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, get the tokens without punctuation and numbers\n",
        "\n",
        "#grep the lines that start with a number (get the tokens)\n",
        "#remove the lines with numbers or punctuation\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"NUM|PUNCT\" | head -50 # looks about right!"
      ],
      "metadata": {
        "id": "xxhfL2_kjiVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check if the numbers were removed by comparing with the original file\n",
        "#and we can see that the number 18.5 is removed indeed\n",
        "#however, 7/x # remains since it is not tagged as NUM, but SYM\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | head -50"
      ],
      "metadata": {
        "id": "ui1VQy4FZ-9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the tokens without numbers and punctuation\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"NUM|PUNCT\" | wc -l"
      ],
      "metadata": {
        "id": "5Lu84Wd8jr16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2c.Count the sentences\n",
        "# to count sentences, we can search for lines with 1 (the first token in each sentence has line number 1)\n",
        "# be sure to match just 1, not 10!\n",
        "# [[:space:]] works with egrep and matches \\t\n",
        "# another option is grep -P, which accepts \\t\n",
        "# surely there can be others as well!\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^1[[:space:]]\" | head -5\n",
        "! echo '---'\n",
        "! zcat covidtweets.conllu.gz| grep -P \"^1\\t\" | head -5"
      ],
      "metadata": {
        "id": "a_QPNIp4j8ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then the sentence count!\n",
        "\n",
        "! zcat covidtweets.conllu.gz| grep -P \"^1\\t\" | wc -l"
      ],
      "metadata": {
        "id": "Q5kaSjQglniW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Then the lexical characteristics, let's start with the lemmas"
      ],
      "metadata": {
        "id": "sHzz4dkVl0jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3a. The most frequent lemma\n",
        "#lemmas are in column number 3\n",
        "\n",
        "#grep the token lines\n",
        "#choose the correct column\n",
        "#and check that you got the lemma column\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | cut -f 3 | head"
      ],
      "metadata": {
        "id": "sEGVCFHFl5PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can just do the frequency list from column 3\n",
        "\n",
        "#after cut -f 3 add the frequency list\n",
        "#and check what the frequency list looks like\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | cut -f 3 | sort | uniq -c | sort -rn | head"
      ],
      "metadata": {
        "id": "IfR7xxR9mcG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3b. Remove function words\n",
        "# let's start with removing numbers and punctuation\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM\" | cut -f 3 | sort | uniq -c | sort -rn | head"
      ],
      "metadata": {
        "id": "XxEqjXkhmnwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there's a lot I don't want to have in the above list.\n",
        "#But it's kinda hard to know what POS categories they are, so first,\n",
        "# I'll just do a frequency list with the lemmas + POS tags"
      ],
      "metadata": {
        "id": "ibKulcXVm_A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grep the token lines\n",
        "#remove punctuation and numbers\n",
        "#take the lemma and pos columns\n",
        "#(N.B. if you use cut -f on several columns, there's no space between the comma and the following number)\n",
        "#and check that you got the correct columns\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM\" | cut -f 3,4 | head"
      ],
      "metadata": {
        "id": "hPl_R3kbnGAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#then do the frequency list of the lemmas and pos tags\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM\" | cut -f 3,4 | sort | uniq -c | sort -rn | head"
      ],
      "metadata": {
        "id": "WvoEP1BwdrTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3b.\n",
        "# so at least SCONJ, AUX, PRON away..."
      ],
      "metadata": {
        "id": "jbIDsqmMnp-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM|PRON|SCONJ|AUX\"  | cut -f 3,4 | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "bt2GP29Un7nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looks so much better! but yet at least CCONJ away...\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM|PRON|SCONJ|CCONJ|AUX\"  | cut -f 3,4 | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "PcM1DRBNoLcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but I think that one does it!"
      ],
      "metadata": {
        "id": "P87aBl-soZyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then the specific POS classes"
      ],
      "metadata": {
        "id": "CnsArWIzp5nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4a. Most frequent lemmas for NOUN\n",
        "\n",
        "#grep the NOUNS\n",
        "#check that you got the NOUNS\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"NOUN\" | head # so these match nouns"
      ],
      "metadata": {
        "id": "FBr8O1gVp9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then just column 3 and the frequencies\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"NOUN\" | cut -f 3 | sort | uniq -c | sort -rn| head -20"
      ],
      "metadata": {
        "id": "krPW1hZ6qIcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4b. Most frequent lemmas for ADJ\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"ADJ\" | cut -f 3 | sort | uniq -c | sort -rn| head -20"
      ],
      "metadata": {
        "id": "2lHdn5oBqWj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4c. Most frequent lemmas for VERB\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"VERB\" | cut -f 3 | sort | uniq -c | sort -rn| head -20"
      ],
      "metadata": {
        "id": "pGeuPnRsqem-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXTRA TIME LEFT?**\n",
        "\n",
        "Choose a word and count the frequencies of the different forms of the word."
      ],
      "metadata": {
        "id": "MgnpZTm9h-b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I decided to have a look at the different adjective compounds with the word 'korona'\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"korona#\" | cut -f 3,4 | egrep \"ADJ\" | sort | uniq -c | sort -nr | head"
      ],
      "metadata": {
        "id": "jlgD1hd14HUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise II\n",
        "\n",
        "Let's do this exercise together.\n",
        "\n",
        "In this exercise we'll use a **Python wrapper**. Let's not go into detail here, since this is not a course in Python, instead it suffices to understand this on the surface.\n",
        "\n",
        "The point is to show how we can use Python scripts in Bash to get more out of our data. In this Python wrapper we use a script that takes **arguments**, and in these arguments we can **specify details** of the Tweets we want to find.\n",
        "\n",
        "With this Python wrapper we can print only the tweets that match the content and time of our query.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "`! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"Covid\" --time \"2020\" `\n",
        "\n",
        "Let's break this down.\n",
        "\n",
        "\n",
        "\n",
        "*   First we print the file `! zcat covidtweets.conllu.gz`\n",
        "*   Then on this file we run the Python script. We need to give the command `python3` and the path to the script `../scripts/read_conllu_docs.py`\n",
        "*   Finally we have two options marked with `--`, in this case `--time` and `--text`\n",
        "\n",
        "\n",
        "  *   These refer to the metadatalines in the data file\n",
        "\n",
        "      `###C:TIME`\n",
        "      \n",
        "      `###C:TEXT`\n",
        "\n",
        "\n",
        "\n",
        "  *   By assigning a specific time or word(s) to these options respectively, we can get Tweets from a specific date/time frame and on a specific topic\n",
        "\n",
        "\n",
        "\n",
        "  *   Both options match any string in the respective metadatalines, so all of the following are valid options:\n",
        "  \n",
        "      `--time \"2021\"`  \n",
        "      `--time \"2021-02\" `\n",
        "\n",
        "      `--text \"Covid\"`  \n",
        "      `--text \"korona on\" `\n",
        "\n",
        "\n",
        "  *   Also, the `--text` option normalises everything to lower case, `so --text \"TWEET\"` matches any upper / lower case versions.\n",
        "\n",
        "Let's see how the Python wrapper works. In the example, we look for tweets with a mention of \"Covid\" from the year 2020."
      ],
      "metadata": {
        "id": "HqM6zOsRq4jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tweets with \"Covid\" from 2020.\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"Covid\" --time \"2020\" | head -40"
      ],
      "metadata": {
        "id": "607-U-SPH9Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we can see that the options match any string\n",
        "#\"aurinko paistaa\" means the sun is shining\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2019-12\" --text \"aurinko paistaa\" | head -40"
      ],
      "metadata": {
        "id": "jfXp14NjOfCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz | egrep \"###C:TIME\" | cut -f 2 -d ' ' | cut -f 1 -d '-' | sort | uniq -c | sort -nr | head"
      ],
      "metadata": {
        "id": "-ICFQQCBPmPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise III\n",
        "\n",
        "\n",
        "Let's focus on tweets that mention specific persons. At least the prime minister @MarinSanna is mentioned frequently, so let's try her.\n",
        "\n",
        "1. How could you first fetch **just the tweets** that mention her? **Direct** those tweets **to a file**.\n",
        "\n",
        "2. a) **How many tweets** does this file have? How are they **distributed over time**?\n",
        "\n",
        "    b) What would be **a reasonable way to analyze the distribution** - the time stamps are quite detailed and not equally distributed?\n",
        "\n",
        "3. Let's try to **compare** the contents of **tweets** mentioning @MarinSanna **at different periods of time**. Ideally, we would have a sufficient number of tweets to compare, representing different time spots of the crisis.\n",
        "\n",
        "    a) Gather the tweets to be compared, and analyze possible differences in terms of **frequent words**. Which POS classes provide the most interpretable results? If any?\n",
        "  \n",
        "    b) Can you see some **differences** that could be **related to the different periods** in the crisis and **events** that took place?\n",
        "\n",
        "    When you find interesting words that could reflect some events, remember to analyze tweets with those words to check if the words are used like you anticipated.\n",
        "\n",
        "4. **If you have time left** after this, you can try with different politicians or **other handles**. For instance, @THL_org seems quite frequent - what is it and what do twitters tweet about it?"
      ],
      "metadata": {
        "id": "GdEOO_4JN8Oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tweets with @MarinSanna to a file\n",
        "#First check what you get with your command\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"@MarinSanna\" | head"
      ],
      "metadata": {
        "id": "JMMXw8bySgk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Then direct to a file\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"@MarinSanna\" | gzip > sannamarin.conllu.gz"
      ],
      "metadata": {
        "id": "XepV5s-kSRr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And check that the file looks as intended\n",
        "\n",
        "! zcat sannamarin.conllu.gz | head -50 # these look ok!"
      ],
      "metadata": {
        "id": "Cu2onssDTIA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.a) How many tweets does this file have?\n",
        "# by grepping and counting the lines starting with ###C:TEXT we can count the number of tweets\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TEXT\" | wc -l"
      ],
      "metadata": {
        "id": "yFVdGxjSTYea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How are they distributed over time?\n",
        "# By grepping and counting the time stamps we can get their distribution over time\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" | sort | uniq -c | sort -rn | head\n",
        "\n",
        "# but clearly these stamps are too detailed, no trends can be seen"
      ],
      "metadata": {
        "id": "CHyCLR1OTvEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.b) What would be a reasonable way to analyze the distribution?\n",
        "# I'll take months (and delete the days and times)\n",
        "\n",
        "# for the dates\n",
        "#cut -f 2 -d ' ' (second column, delimiter white space)\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" |  cut -f 2 -d ' '| head"
      ],
      "metadata": {
        "id": "g7Wno0mlUguX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the months\n",
        "\n",
        "#cut -f 2 -d ' ' | cut -f 1,2 -d '-'\n",
        "# first take the dates, then from the dates, take columns 1 and 2 (year and month) delimited by a hyphen\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" |  cut -f 2 -d ' '| cut -f 1,2 -d '-' | head # this looks ok!"
      ],
      "metadata": {
        "id": "yvCE-KNfVETr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to make sure, let's see how the distribution is\n",
        "#i.e. make a frequency list\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" |  cut -f 2 -d ' '| cut -f 1,2 -d '-' | sort | uniq -c | sort -rn | head -30\n",
        "\n",
        "# or, actually, I think the distribution is kinda wonky, mostly just from the first months of the crisis."
      ],
      "metadata": {
        "id": "aPyQt-vbVNSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's try to compare the contents of tweets mentioning @MarinSanna at different periods of time.\n",
        "\n",
        "# what about the first days of the crisis? What would be frequent enough?\n",
        "# Based on this frequency list, I'll take March 17 and March 18\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" | egrep \"2020-03\" | cut -f 2 -d ' ' | cut -f 1,2,3 -d '-'| sort | uniq -c | sort -rn | head -30"
      ],
      "metadata": {
        "id": "TwrG2N-BjanK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unfortunately my python script doesn't support regexes\n",
        "#let's gather the tweets from the chosen dates to two files\n",
        "\n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-03-18\" > 03-18.conllu\n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-03-17\" > 03-17.conllu"
      ],
      "metadata": {
        "id": "bTEHvDd7lDxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to check the numbers match - looks like ok!\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu | egrep \"###C: NEWDOC\" | wc -l\n",
        "! cat 03-18.conllu 03-17.conllu | egrep \"###C:TIME\" | cut -f 2 -d ' ' | cut -f 1,2,3 -d '-' | sort | uniq -c | sort -rn"
      ],
      "metadata": {
        "id": "jx_cpDpplfn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "7Y-NUfiV-Ohs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! head -20 03-18.conllu\n",
        "! echo '---'\n",
        "! head -20 03-17.conllu"
      ],
      "metadata": {
        "id": "BrP3e--0-QQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! head *conllu"
      ],
      "metadata": {
        "id": "emszWEQs-cfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.a) Analyze possible differences in terms of frequent words.\n",
        "#Which POS classes provide the most interpretable results?\n",
        "\n",
        "#Let's first check the frequencies of POS tags\n",
        "#N.B. Unless you remove the metadata lines before cutting the column you are interested in,\n",
        "#the metadata lines will be part of your frequency list\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu  | egrep -v \"^###|^$\" | cut -f 4 | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "imt6cOZH-pBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.a) Analyze possible differences in terms of frequent words.\n",
        "#Which POS classes provide the most interpretable results?\n",
        "\n",
        "# let's check the most frequent adjectives, nouns, verbs\n",
        "\n",
        "#get the lemmas of the selected POS tags:\n",
        "#print the file, remove metadata and empty lines, grep the lines with the POS tags, cut the lemma column, frequency list\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu  | egrep -v \"^###|^$\" | egrep \"ADJ|NOUN|VERB\" | cut -f 3 | sort | uniq -c | sort -rn | head -20\n",
        "\n",
        "# I guess these are ok, but I'm not sure about the verbs. would the list be better without verbs?"
      ],
      "metadata": {
        "id": "vclBja4imt6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's leave the verbs out\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu  | egrep -v \"^###|^$\" | egrep \"ADJ|NOUN\" | cut -f 3 | sort | uniq -c | sort -rn | head -20\n"
      ],
      "metadata": {
        "id": "wQJ4BfClocjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# well, I guess these do reflect some moments in the crisis!\n",
        "# there's valmiuslaki \"emergency powers legislation\", tärkeä \"important\", raja \"border\"\n",
        "# I'll yet check how these are used in the tweets. Unf. my script doesn't search for lemmas,\n",
        "#but maybe the string will get some matches anyway!\n",
        "\n",
        "#to get the texts only, egrep \"###C:TEXT\" after the option\n",
        "\n",
        "#N.B. two files can be read simultaneously by listing them after cat\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu  | python3 ../scripts/read_conllu_docs.py --text \"raja\" | egrep \"###C:TEXT\" | head\n",
        "\n",
        "# well, not horrible! The tweets mostly seem to discuss borders and border control"
      ],
      "metadata": {
        "id": "yMcGivZFohZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the comparison to April. Again, we need enough tweets, preferably from late April so there's some time gap to the March tweets we have. Let's see how many tweets we have from late April"
      ],
      "metadata": {
        "id": "kMHZAl_9pkoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.b) Can you see some differences that could be related to the different periods in the crisis and events that took place?\n",
        "# these are now just the days of April\n",
        "\n",
        "#cut -f 2 -d ' ' | egrep \"2020-04\" | cut -f 3 -d '-'\n",
        "#take the year-month-day column, then grep April in 2020, and finally take the day column separated by a hyphen\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"###C:TIME\" | cut -f 2 -d ' ' | egrep \"2020-04\" | cut -f 3 -d '-' | sort | uniq -c | sort -rn\n",
        "\n",
        "# not many tweets from late April, but let's take April 15 and 14"
      ],
      "metadata": {
        "id": "kumNSsIDr6ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#direct the tweets from the chosen dates to files\n",
        "\n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-04-15\" >  04-15.conllu\n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-04-14\" >  04-14.conllu"
      ],
      "metadata": {
        "id": "nMI52epLXygc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "e4fN7q9YClyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! head 04-14.conllu\n",
        "! echo '---'\n",
        "! head 04-15.conllu"
      ],
      "metadata": {
        "id": "UB74z_wNCnXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the same POS for April as for March\n",
        "\n",
        "#N.B. cat 04* matches all files that start with 04, so in this case both April files\n",
        "\n",
        "! cat 04*  | egrep -v \"^###|^$\" | egrep \"ADJ|NOUN\" | cut -f 3 | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "50vi0hlolcez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to chech which files you match with the *\n",
        "#you can use ls which will list you all the files that match the given pattern\n",
        "\n",
        "! ls 04*"
      ],
      "metadata": {
        "id": "9KP1ByiMC3uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I guess there's some differences, at least mökki \"summer house\" and maski \"mask\". Let's see how these tweets with these words look like."
      ],
      "metadata": {
        "id": "L-KkpLE1uOUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the words in the tweets\n",
        "\n",
        "! cat 04* | python3 ../scripts/read_conllu_docs.py --text \"mökki\" | egrep \"###C:TEXT\" | head\n",
        "\n",
        "# well, looks like I should I'd think!"
      ],
      "metadata": {
        "id": "V2z2gN6_umfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Extra.\n",
        "#THL_org seems quite frequent - what is it and what do twitters tweet about it?\n",
        "\n",
        "#fetch the tweets that have the handle\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"@THLorg\" | head"
      ],
      "metadata": {
        "id": "vPAIav5KFpZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#direct these tweets to a new zipped file\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"@THLorg\" | gzip > thl_tweets.gz.conllu"
      ],
      "metadata": {
        "id": "aT7rU1OjGYEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check that succeeded\n",
        "\n",
        "! ls"
      ],
      "metadata": {
        "id": "jgbtJrxiGtUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check that the new file looks ok\n",
        "\n",
        "! zcat thl_tweets.gz.conllu | head"
      ],
      "metadata": {
        "id": "aUE9-ay1HVGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count the texts in the file\n",
        "\n",
        "! zcat thl_tweets.gz.conllu | egrep \"^###C: NEWDOC\" | wc -l"
      ],
      "metadata": {
        "id": "9wxXCucpGTS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What are these tweets about?\n",
        "\n",
        "#remove metadata and empty lines\n",
        "#fetch the lemma and POS columns\n",
        "#remove punctuation marks\n",
        "#fetch the lemma column; N.B. now the lemmas are in the first column since you cut from the output of the previous command\n",
        "#in which you selected two columns\n",
        "#normalize to lower case\n",
        "#remove any handles\n",
        "#make a frequency list\n",
        "#function words are most common, so checking a bit further down in the frequency list is perhaps more interesting\n",
        "\n",
        "! zcat thl_tweets.gz.conllu | egrep -v \"^###C:|^$\" | cut -f 3,4 | egrep -v \"PUNCT\" | cut -f 1 | tr '[:upper:]' '[:lower:]' | egrep -v \"^@\" | sort | uniq -c | sort -nr | head -150 | tail -50"
      ],
      "metadata": {
        "id": "wQUqT0zbIXcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How could you find other frequent handles in the texts?\n",
        "\n",
        "! zcat covidtweets.conllu.gz | egrep -v \"^###\" | cut -f 2 | egrep \"^@\" | sort | uniq -c | sort -nr | head"
      ],
      "metadata": {
        "id": "Qiv5WJtoFsiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Summary of useful commands in these exercises**\n",
        "\n",
        "*   match **tab** in egrep\n",
        "\n",
        "    `egrep \"^1[[:space:]]\"`\n",
        "\n",
        "*   grep lines with ADJ, NOUN, **or** VERB\n",
        "\n",
        "    `egrep \"ADJ|NOUN|VERB\"`\n",
        "\n",
        "\n",
        "*   get **several columns**; N.B. no white space between the column numbers\n",
        "\n",
        "    `cut -f 3,4`\n",
        "\n",
        "*   get a **column separated by** a hyphen; `d` in `-d ' '` stands for delimiter, while the delimiter comes in single quotation marks; the default delimiter is tab\n",
        "\n",
        "\n",
        "    `cut -f 3 -d '-'`\n",
        "\n",
        "\n",
        "*   print **several files simulatenously**\n",
        "\n",
        "    `cat 03-18.conllu 03-17.conllu`\n",
        "\n",
        "\n",
        "*   print all files **starting** with 04\n",
        "\n",
        "    `cat 04*`\n",
        "\n",
        "\n",
        "*   print all files **ending** with `.txt`\n",
        "\n",
        "    `cat *.txt`\n",
        "\n",
        "\n",
        "\n",
        "*   **list** all files ending in `.txt`\n",
        "\n",
        "    `ls *.txt`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aXOg3tt9afaU"
      }
    }
  ]
}