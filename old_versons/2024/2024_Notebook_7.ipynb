{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/2024_Notebook_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise I\n",
        "\n",
        "In the Github repo https://github.com/TurkuNLP/ATP_kurssi.git there are syntax analyzed tweets in a folder called 'data'. The file is called simply `covidtweets.conllu.gz`. Let's work on this a bit.\n",
        "\n",
        "### 1. Preparations\n",
        "\n",
        "\n",
        "*   a. clone the repo\n",
        "*   b. go to the folder\n",
        "\n",
        "\n",
        "\n",
        "### 2. Basics\n",
        "* a. How many tweets?\n",
        "* b. How many tokens? What if you exclude punctuation and numbers?\n",
        "* c. How many sentences? Note that `\\t` (tab which separates the columns in the file) does not work with egrep, can you google for how to do this?\n",
        "\n",
        "### 3. Lexical characteristics\n",
        "* a. The most frequent lemmas?\n",
        "* b. What if you exclude function words? The definition of function words can vary a bit. What do you think could be the most useful POS classes to keep to get a general view to the contents of the tweets?\n",
        "* Note: it might be hard to figure out the POS tags associated with the words, you can also analyze this by combining the lemma and POS columns\n",
        "\n",
        "### 4. More\n",
        "* Now that we have POS classes, we can also focus on specific kinds of words. So let's count the most frequent lemmas for\n",
        "  * a. nouns (NOUN)\n",
        "  * b. adjective (ADJ)\n",
        "  * c. verbs (VERB)\n",
        "* Which POS class words provide the most interesting results in your opinion?\n",
        "\n",
        "(I've provided you with cells for the tasks, but you might need more of them!)"
      ],
      "metadata": {
        "id": "xx16JAjMekGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.a) Clone the repo"
      ],
      "metadata": {
        "id": "r6gqAPvBhDeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1.b) Go to the folder"
      ],
      "metadata": {
        "id": "pqOC1KFNhDVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the file"
      ],
      "metadata": {
        "id": "6yR-bphZhDMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.a) Count the tweets"
      ],
      "metadata": {
        "id": "2n9GcMAihDCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.b) Count the tokens"
      ],
      "metadata": {
        "id": "AD6XhBeihC5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.b) Remove punctuation and numbers and count the tokens"
      ],
      "metadata": {
        "id": "8QuHlz9thCvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.c) Count the sentences"
      ],
      "metadata": {
        "id": "SEyJK6nahCkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.a) The most frequent lemma"
      ],
      "metadata": {
        "id": "vO4ejbOfhCK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.b) Remove function words"
      ],
      "metadata": {
        "id": "jD-29MchiD_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.a) Most frequent lemmas for NOUN"
      ],
      "metadata": {
        "id": "W_QyzegaiGT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.b) Most frequent lemmas for ADJ"
      ],
      "metadata": {
        "id": "fviPa8ULiteJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.c) Most frequent lemmas for VERB"
      ],
      "metadata": {
        "id": "wkCWMYdvivis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise II\n",
        "\n",
        "In this exercise we'll use a **Python wrapper**. Let's not go into detail here, since this is not a course in Python, instead it suffices to understand this on the surface.\n",
        "\n",
        "The point is to show how we can use Python scripts in Bash to get more out of our data. In this Python wrapper we use a script that takes **arguments**, and in these arguments we can **specify details** of the Tweets we want to find.\n",
        "\n",
        "With this Python wrapper we can print only the tweets that match the content and time of our query.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "`! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"Covid\" --time \"2020\" `\n",
        "\n",
        "Let's break this down.\n",
        "\n",
        "\n",
        "\n",
        "*   First we print the file `! zcat covidtweets.conllu.gz`\n",
        "*   Then on this file we run the Python script. We need to give the command `python3` and the path to the script `../scripts/read_conllu_docs.py`\n",
        "*   Finally we have two options marked with `--`, in this case `--time` and `--text`\n",
        "\n",
        "\n",
        "  *   These refer to the metadatalines in the data file\n",
        "\n",
        "      `###C:TIME`\n",
        "      \n",
        "      `###C:TEXT`\n",
        "\n",
        "\n",
        "\n",
        "  *   By assigning a specific time or word(s) to these options respectively, we can get Tweets from a specific date/time frame and on a specific topic\n",
        "\n",
        "\n",
        "\n",
        "  *   Both options match any string in the respective metadatalines, so all of the following are valid options:\n",
        "  \n",
        "      `--time \"2021\"`  \n",
        "      `--time \"2021-02\" `\n",
        "\n",
        "      `--text \"Covid\"`  \n",
        "      `--text \"korona on\" `\n",
        "\n",
        "\n",
        "  *   Also, the `--text` option normalises everything to lower case, `so --text \"TWEET\"` matches any upper / lower case versions.\n",
        "\n",
        "Let's see how the Python wrapper works. In the example, we look for tweets with a mention of \"Covid\" from the year 2020.\n"
      ],
      "metadata": {
        "id": "HqM6zOsRq4jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tweets with \"Covid\" from 2020.\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"Covid\" --time \"2020\" | head -40"
      ],
      "metadata": {
        "id": "uHo-aZ8gH90d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we can see that the options match any string\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2019-12\" --text \"aurinko paistaa\" | head -40"
      ],
      "metadata": {
        "id": "FlEzF98iIBPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercises**\n",
        "\n",
        "\n",
        "Let's focus on tweets that mention specific persons. At least the prime minister @MarinSanna is mentioned frequently, so let's try her.\n",
        "\n",
        "1. How could you first fetch just the tweets that mention her? Direct those tweets to a file.\n",
        "\n",
        "2. a) How many tweets does this file have? How are they distributed over time?\n",
        "b) What would be a reasonable way to analyze the distribution - the time stamps are quite detailed and not equally distributed?\n",
        "\n",
        "3. Let's try to compare the contents of tweets mentioning @MarinSanna at different periods of time. Ideally, we would have a sufficient number of tweets to compare, representing different time spots of the crisis.\n",
        "\n",
        "  a) Gather the tweets to be compared, and analyze possible differences in terms of frequent words. Which POS classes provide the most interpretable results? If any?\n",
        "  \n",
        "  b) Can you see some differences that could be related to the different periods in the crisis and events that took place?\n",
        "\n",
        "  When you find interesting words that could reflect some events, remember to analyze tweets with those words to check if the words are used like you anticipated.\n",
        "\n",
        "4. If you have time left after this, you can try with different politicians or other handles. For instance, @THL_org seems quite frequent - what is it and what do twitters tweet about it?"
      ],
      "metadata": {
        "id": "LVGwOsfsHm8j"
      }
    }
  ]
}
