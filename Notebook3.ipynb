{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+RE/yYweXcgwYp5QzYB8h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/Notebook3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9-ebWc10sMh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqTKjj3w0zVM"
      },
      "source": [
        "\n",
        "## Copying a Github repo\n",
        "\n",
        "* Github is a common place to save code and data in NLP.\n",
        "* The repos (directories) can be copied to a local computer programatically\n",
        "* This is quite handy especially with Google colab\n",
        "* The command for the copying is *git clone*, and it should be followed the url \"Code\" link in the green box available at a Git repo\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVWyz9Eh04FT"
      },
      "source": [
        "! git clone https://github.com/TurkuNLP/ATP_kurssi.git\n",
        "! ls #to check that we got the repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4-bZa7906Pz"
      },
      "source": [
        "# cd will take us to that folder\n",
        "%cd ATP_kurssi/\n",
        "! ls # check that we are at the correct place"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvHGpOKV08Gz"
      },
      "source": [
        "### Exercise with tweets \n",
        "\n",
        "* The file tweets_en_nort.csv includes tweets\n",
        "* How many are there?\n",
        "* What does the last tweet look like?\n",
        "* Can you think what they discuss? What seems the best way to read the data?\n",
        "* The dataset has a lot of bot-generated tweets that we'd like to filter out. At least one such tweet has the string \"Message of Leader\". Filter those out. How many such tweets were there? How many tweets do you have in the \"cleaned\" dataset after the filtering?\n",
        "* (There can also be other repetitive tweets, if you see any, you can take those away too)\n",
        "\n",
        "* The tweets mention at least France and Paris. Direct these to a separate file, which includes all spelling variants.\n",
        "* Can you find tweets where France or Paris are not capitalized?\n",
        "* There seems to be many news agencies. How many times is FoxNews mentioned? What about other news agencies, can you see any?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mepYjuJG2BTj"
      },
      "source": [
        "## Frequency counts\n",
        "\n",
        "* A frequency counter can be done by combining *sort* and *uniq* commands \n",
        "* Sort sorts the input lines to alphabetical order\n",
        "* Uniq filters repetitive lines\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50WaebTa2rcB"
      },
      "source": [
        "! head -5 en_one_per_line.txt  # let's have a look first\n",
        "! wc -l en_one_per_line.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nimgfyu6n_S"
      },
      "source": [
        "! cat en_one_per_line.txt| sort "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2rodHfs60fs"
      },
      "source": [
        "! cat en_one_per_line.txt| sort | uniq # this will only print once the repetitive lines!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUONqqZ87CnZ"
      },
      "source": [
        "# we can check this by counting how many lines we have"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-IJv-KQ7FrB"
      },
      "source": [
        "! cat en_one_per_line.txt| sort | wc -l\n",
        "! cat en_one_per_line.txt| sort | uniq | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlOIAfGE7VRx"
      },
      "source": [
        "### Uniq -c\n",
        "In addition to deleting repeated items, uniq -c counts them\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUyGOOvo7IzT"
      },
      "source": [
        "! cat en_one_per_line.txt| sort | uniq -c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW9lhqU67hrB"
      },
      "source": [
        "### Sorting uniq -c list by frequency\n",
        "\n",
        "* By adding another sort after uniq -c, we get a sorted frequency list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSNq6f6O78Bi"
      },
      "source": [
        "! cat en_one_per_line.txt| sort | uniq -c | sort"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrwjmrb477kK"
      },
      "source": [
        "### BUT sort alone does not understand numbers\n",
        "\n",
        "* Sort -n does"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeskkW6Y8PZD"
      },
      "source": [
        "! cat en_one_per_line.txt| sort | uniq -c | sort -n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSjf2e7e8Vab"
      },
      "source": [
        "# Note that we only see the last lines unless we use head\n",
        "! cat en_one_per_line.txt| sort | uniq -c | sort -n | head -5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgTnU4dP83rs"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "* Let's use the same `en_one_per_line.txt` file\n",
        "* Sort the file first by *sort*. \n",
        "* Then filter out duplicates with *uniq*. Print the 5 first tokens.\n",
        "* How many unique strings (words etc) do you have in the file?\n",
        "* Make a frequency list of the words. What are the 10 most frequent tokens? With how many occurrences? \n",
        "* Create a file that has a frequency list of the words.\n",
        "\n",
        "### Advanced\n",
        "* Find out (from google or smth else) what sort option can be used to reverse the sorting so that the most frequent ones come first?\n",
        "* Find the words that are used only once. How many such words does the data include?\n",
        "* Create a frequency list file from where you have filtered out words that occur only once.\n",
        "\n"
      ]
    }
  ]
}