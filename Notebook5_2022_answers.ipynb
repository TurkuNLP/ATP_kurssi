{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/Notebook5_2022_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        "\n",
        "* Let's use the CORE-corpus availablet at https://github.com/TurkuNLP/CORE-corpus. Clone it to your notebook. Focus on the development set of the dataset. How many documents does it include?\n",
        "\n",
        "* Column 1 indicates the register abbreviations of the documents. They are explained in the txt-file found in the repo. What are the 10 most frequent register combinations found in the dataset? What actual classes do these abbreviations correspond to?\n",
        "\n",
        "* Select two registers you want to focus on. How to make a regular expression that matches the register label in the first column but not the actual text?  Make files that consist of frequency lists of the words of these two registers. Normalize the words first to lower case and delete numbers and punctuation. Can you think of how to combine the delete commands to one using regexes?\n",
        "\n",
        "* How many unique words do the two registers have?\n",
        "\n",
        "* How would you search for words that occur at least 100 times using regex? Note that the frequency list has sometimes whitespaces in the beginning before the numbers. See how fast the frequency of the words drops!\n",
        "\n",
        "* Have a look at the most frequent words of the registers - again, they are super-similar! To have a better idea of the differences of the two files and the texts they represent, we can use a python script that will count the keywords of the files. These are words that are statistically overrepresented in the studied file / register in comparison with another one. \n",
        "\n",
        "The keyword method is described in the following paper: \n",
        "```\n",
        "Incorporating text dispersion into keyword analyses\n",
        "Jesse Egbert and Doug Biber\n",
        "Corpora 2019 14:1, 77-104\n",
        "```\n",
        "\n",
        "* You can find a script for extracting keywords in the `scripts` folder of the course Github repo. So by cloning the repo, you will also get the script. You can run the script by\n",
        "\n",
        "```\n",
        "python3 ATP_kurssi/scripts/text_dispersion_text.py file1 file2 \n",
        "```\n",
        "\n",
        "And it will give you the keywords of the first file. Do these make sense? If you have time, you can also extract two other register classes and see how their keywords look like.\n",
        " \n",
        "### Extra\n",
        "If you have time, try to modify the data so that instead of words you have word two-grams (combinations of two words such as _two-words_ _words-such _such-as_) and then run key two-grams instead of keywords. How do those look like?"
      ],
      "metadata": {
        "id": "_hV8Po1VAmCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/TurkuNLP/CORE-corpus.git"
      ],
      "metadata": {
        "id": "-k-zafibBng3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cec42ad-b4f5-4c43-9e7d-2483d07ec558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CORE-corpus'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 25 (delta 8), reused 5 (delta 2), pack-reused 8\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd CORE-corpus"
      ],
      "metadata": {
        "id": "8bYvrLFEBu5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat dev.tsv.gz | head # how does it look like?"
      ],
      "metadata": {
        "id": "iLgBD2GOBvrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat dev.tsv.gz | wc -l # one document per line, so by counting lines we get the number of documents"
      ],
      "metadata": {
        "id": "NtXU1EePB95g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat dev.tsv.gz | cut -f 1 | sort | uniq -ci | sort -rn | head -10 # focus on the column 1, make a frequency list"
      ],
      "metadata": {
        "id": "QBjSYgd3CF8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat register_label_abbreviations.txt | egrep \"NA|NE|OP|OB|IN|DT|SR|ID|DF|IN|RV|IB|HI|HT\" # this will give us explanations for the labels"
      ],
      "metadata": {
        "id": "HcgLKeEvDU3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> So it would seem that Narrative + news article texts are the most frequent in the CORE corpus, followed by Opinion + opinion blogs\n",
        "\n",
        "I will compare personal blogs (pb) and travel blogs (tb), I want to understand what's the difference between these two!"
      ],
      "metadata": {
        "id": "zTr-If1GDnnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat dev.tsv.gz | egrep \"^[A-Z]+ PB\" | head  # beginning of line (^), any uppercase letter [A-Z] one or more times (+), whitespace,  PB\n"
      ],
      "metadata": {
        "id": "2Wyj8VczDkuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat dev.tsv.gz | egrep \"^[A-Z]+ PB\" | cut -f 3 > pb.txt # I direct these two a file for simplicity"
      ],
      "metadata": {
        "id": "LCPlrBLdBAHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat pb.txt | tr '[:upper:]' '[:lower:]' | tr '[0-9]|[:punct:]' ' ' | tr ' ' '\\n' | head -100 # how we have all one per line and normalized "
      ],
      "metadata": {
        "id": "KNVKTubv_Vtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat pb.txt | tr '[:upper:]' '[:lower:]' | tr '[0-9]|[:punct:]' ' ' | tr ' ' '\\n' | egrep -v \"^$\" | sort | uniq -ci | sort -rn > pb-freks.txt # also empty lines away!"
      ],
      "metadata": {
        "id": "SW_zguxrAd7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's run the same pipeline now for travel blogs\n",
        "! zcat dev.tsv.gz | egrep \"^[A-Z]+ TB\" | cut -f 3 > tb.txt\n",
        "! cat tb.txt | tr '[:upper:]' '[:lower:]' | tr '[0-9]|[:punct:]' ' ' | tr ' ' '\\n' | egrep -v \"^$\" | sort | uniq -ci | sort -rn > tb-freks.txt\n"
      ],
      "metadata": {
        "id": "zfif2KPRHx_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat tb-freks.txt | head -5 # to make sure the file looks ok"
      ],
      "metadata": {
        "id": "2_XD2M-aIcE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat pb-freks.txt | wc -l # how many unique words\n",
        "! cat tb-freks.txt | wc -l\n",
        "# or even\n",
        "! wc -l *freks.txt"
      ],
      "metadata": {
        "id": "249ILl9PEWay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how many occur at least 100 times?\n",
        "! cat pb-freks.txt | egrep \"^ *[0-9][0-9][0-9] \" | wc -l # another option would be \"^ *[0-9]{3} \" where {} denote how many numbers we want"
      ],
      "metadata": {
        "id": "fltaAFlwEhoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat tb-freks.txt | head -150 | tail -50 # the beginning is so uninformative I wanted to look at the ranks 100-150"
      ],
      "metadata": {
        "id": "gPxjIn95GGCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ! git clone https://github.com/TurkuNLP/ATP_kurssi.git"
      ],
      "metadata": {
        "id": "aMN78-zSNTJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls ATP_kurssi/scripts/ # here are the scripts we want to use"
      ],
      "metadata": {
        "id": "UUCO_EzCqD96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the keyness script takes as arguments the two files we want to compare, and prints the keywords for the first file\n",
        "! python3 ATP_kurssi/scripts/text_dispersion_text.py tb.txt pb.txt | head -40"
      ],
      "metadata": {
        "id": "RxBeeEQPqKmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "\n",
        "Let's work on some Tweets. Unfortunately these are in Finnish, but use the file at https://dl.turkunlp.org/atp/all_tweets_one_line.txt.gz\n",
        "\n",
        "Basic stats\n",
        "* How many tweets?\n",
        "* Can you figure out what the tweets are about?\n",
        "* When have they been Tweeted?\n",
        "* Can you make a distribution of how many tweets per month the data has? The `cut` command can be useful for this. `-d ` can specify the delimiter, so e.g.,  `cut -f 1,2 -d ' '` will get the first and second columns separated by a whitespace.\n",
        "\n",
        "Contents\n",
        "* Let's count the most frequent words of the tweets \n",
        "* And also the most important hashtags. And user names.\n",
        "These should tell us what we have in the data!\n",
        "\n",
        "Keywords by time and / or person?\n",
        "\n",
        "* Let's count username or time-related keywords with the python above script. How do the keywords look like, can you trace anything interesting?\n"
      ],
      "metadata": {
        "id": "RUH-e4nF_1az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://dl.turkunlp.org/atp/all_tweets_one_line.txt.gz"
      ],
      "metadata": {
        "id": "UvQmvCbsqW5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | head -5 #again a tweet per line!\n",
        "! zcat all_tweets_one_line.txt.gz | wc -l # this many tweets"
      ],
      "metadata": {
        "id": "pHFyOou02l9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it would seem that the column 3 has the time stamps. Let's see how the head and tail look like\n",
        "! echo \"head\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | head \n",
        "! echo \"tail\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | tail \n",
        "\n",
        "#Based on these, we could guess that the time stamps range from March 2020 to August 2021. But better yet sort the time stamps and then check the head and tail!"
      ],
      "metadata": {
        "id": "wMeBXQzW37p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"here's the head\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | sort | head \n",
        "! echo \"here's the tail\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | sort | tail\n",
        "# oups, in fact the tweets start from 2019 and continue until August 2021. We almost missed the beginning!"
      ],
      "metadata": {
        "id": "qhlhfoOW4wl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | head"
      ],
      "metadata": {
        "id": "1Hiyfkaz5QMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | cut  -f 1 -d ' ' | head # this gives us the dates"
      ],
      "metadata": {
        "id": "7FXSrMLV5-6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3  | cut  -f 1 -d ' ' | cut -f 1,2  -d '-' | head # and here we get just the years and months!"
      ],
      "metadata": {
        "id": "N7RK4ocZ8n98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3  | cut  -f 1 -d ' ' | cut -f 1,2  -d '-' | sort | uniq -c | sort -rn # and by making a freq. list of these, we get the distribution!\n",
        "\n",
        "# so it seems that April 2020 has clearly the most tweets, and 2019 the least"
      ],
      "metadata": {
        "id": "EsKhbzDt82UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | head # so the tweets were in column 10\n",
        "! echo \"just checking\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 10 | head # yeah, this one!"
      ],
      "metadata": {
        "id": "WZ01SK5p9bRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 10 | tr '[:punct:]|[0-9]' ' ' | tr ' ' '\\n' | sort | uniq -c | sort -rn | head -20 # so here's the most frequent words. quite a big data, so takes some time!"
      ],
      "metadata": {
        "id": "JDi9bobe9le4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking all punctuation was maybe not the brightest idea, it takes away the hashtags as well... \n",
        "# so let's take just the hashtags and see those\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 10 | tr ' ' '\\n' |egrep \"^#\" | sort | uniq -c | sort -rn | head -20 "
      ],
      "metadata": {
        "id": "zvk_Qb5oHZex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and usernames\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 10 | tr ' ' '\\n' |egrep \"^@\" | sort | uniq -c | sort -rn | head -20 "
      ],
      "metadata": {
        "id": "IV-3BWbkIKGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> So these are clearly (at least to some extent) related to Covid.\n",
        "--> The most frequent username is THLorg, followed by news agencies and politicians"
      ],
      "metadata": {
        "id": "SzFhFFyaIn7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3,10 | egrep \"^2019\" | cut -f 2 > 2019_tweets.txt\n",
        "\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3,10 | egrep \"^2020-08\" | cut -f 2 > 2020_tweets.txt"
      ],
      "metadata": {
        "id": "daO_LmCQIamU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! head *tweets.txt # looks about right!"
      ],
      "metadata": {
        "id": "GW4O9NcEJTZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python3 ATP_kurssi/scripts/text_dispersion_text.py 2020_tweets.txt 2019_tweets.txt  | head -20"
      ],
      "metadata": {
        "id": "hiMnOlXSJvjn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}