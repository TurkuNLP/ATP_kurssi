{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/ATP_2025_Notebook_5_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on work\n",
        "\n",
        "### Exercise 1\n",
        "\n",
        "* Anna works with Finnish parliamentary speeches and has provided us with some more parliamentary data. Let's try to wrangle a file containing data from year 2019. NB! This file has been pre-cleaned a little to make it work here!\n",
        "\n",
        "* We will be examining the file which contains all speeches given in year 2019: https://raw.githubusercontent.com/aristila/totally-random-stuff/refs/heads/main/finparlspeeches_2019.csv\n",
        "\n",
        "\n",
        "Your tasks:\n",
        "\n",
        "1. Get the file.\n",
        "2. Check what the contents look like.\n",
        "3. One row has one speech and related metadata. How many speeches have been given this year?\n",
        "4. Some interesting columns are \"date\" (the day the speech was given), \"party\" (parliamentary faction), \"lang\" (language the speech is given in; either Finnish, Swedish, or both), and \"content\" which contains the actual speech. These columns are numbers **4, 6, 7, and 9.** Cut these four columns, (Hint: If you don't know how to cut multiple columns at once, Google it!), then save the truncated dataset as a new file called \"**speeches_2019_4c.csv**\". Use this file from now on.\n",
        "5. How many unique parties (parliamentary factions) appear in the data?\n",
        "6. Which party has given the most speeches?\n",
        "7. How many of the speeches are given only in Swedish? How many percentages is that?\n",
        "8. How many times does the word \"virus\" appear in the speeches? What about \"korona\" or \"covid\"?\n",
        "9. Get all the speeches that mention \"ilmastonmuutos\" (climate change). Which party has the most of those?\n",
        "10. On which date were there most speeches about \"ilmastonmuutos\"?\n",
        "\n",
        "EXTRA: Formulate a question of your own! What would you like to find out?"
      ],
      "metadata": {
        "id": "1APs5gVvh14Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get the file.\n",
        "! wget 'https://raw.githubusercontent.com/aristila/totally-random-stuff/refs/heads/main/finparlspeeches_2019.csv'"
      ],
      "metadata": {
        "id": "oCwCo--Th1eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Check what the contents look like.\n",
        "! cat finparlspeeches_2019.csv | head"
      ],
      "metadata": {
        "id": "QLHnMlgbcGLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. One row has one speech and related metadata. How many speeches have been given this year?\n",
        "! cat finparlspeeches_2019.csv | wc -l"
      ],
      "metadata": {
        "id": "jNqRgs4IcLf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Some interesting columns are \"date_\" (the day the speech was given), \"party__prefLabel\" (parliamentary faction),\n",
        "# \"language\" (language the speech is given in; either Finnish, Swedish, or both), and \"content\" which contains\n",
        "# the actual speech. These columns are numbers 4, 6, 7, and 9. Cut these four columns, (Hint: If you don't\n",
        "# know how to cut multiple columns at once, Google it!), then save the truncated dataset as a new file called\n",
        "# \"speeches_2019_4c.csv\". Use this file from now on.\n",
        "\n",
        "# NB! it says 'csv' but the separator is a pipe character (|), define it for cut with -d '|'\n",
        "# (you can Google an example of this as well)\n",
        "! cat finparlspeeches_2019.csv | cut -d '|' -f 4,6,7,9 | head  # check that we got what we wanted\n",
        "! cat finparlspeeches_2019.csv | cut -d '|' -f 4,6,7,9 > speeches_2019_4c.csv\n",
        "! echo '------------------------'\n",
        "! ls  # check that we have it"
      ],
      "metadata": {
        "id": "RFsvuoYpcLc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. How many unique parties (parliamentary factions) appear in the data?\n",
        "! cat speeches_2019_4c.csv | cut -d '|' -f 1 | sort | uniq\n",
        "\n",
        "# NB! There are speeches with empty party labels. Also, the column name is taken as one party here...\n",
        "! cat speeches_2019_4c.csv | cut -d '|' -f 1 | sort | uniq | wc -l"
      ],
      "metadata": {
        "id": "q9TMqy3gcLak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Which party has given the most speeches?\n",
        "! cat speeches_2019_4c.csv | cut -d '|' -f 1 | sort | uniq -c | sort -rn"
      ],
      "metadata": {
        "id": "JQcpUA6Jca_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. How many of the speeches are given only in Swedish?\n",
        "! cat speeches_2019_4c.csv | cut -d '|' -f 2 | sort | uniq -c | sort -rn # let's see what values we have here\n",
        "# only 99 swe out of 13018\n",
        "\n",
        "# How many percentages is that? (Google if there is a way to make Bash do calculations with decimals ;) )\n",
        "# Bash doesnâ€™t support floating-point arithmetic, so if we want to know how many percentages that is, use a calculator ;)"
      ],
      "metadata": {
        "id": "pZYhkwDEca46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. How many times does the word \"virus\" appear in the speeches? What about \"korona\" or \"covid\"?\n",
        "\n",
        "# NB! cutting the column with the speeches is optional here,\n",
        "# since the other columns would not interfere with the  search in any way\n",
        "# but I've cut it anyway just to practice\n",
        "! cat speeches_2019_4c.csv | cut -d '|' -f 4 | egrep -i 'virus' | wc -l\n",
        "! cat speeches_2019_4c.csv | cut -d '|' -f 4 | egrep -i 'korona' | wc -l\n",
        "! cat speeches_2019_4c.csv | cut -d '|' -f 4 | egrep -i 'covid' | wc -l\n"
      ],
      "metadata": {
        "id": "xmZUmGMTca1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Get all the speeches that mention \"ilmastonmuutos\" (climate change). Which party has the most of those?\n",
        "\n",
        "# Here we should NOT cut -f 4 before making the search, so that we still have the party column\n",
        "! cat speeches_2019_4c.csv | egrep -i 'ilmastonmuutos' | wc -l\n",
        "! cat speeches_2019_4c.csv | egrep -i 'ilmastonmuutos' | cut -d '|' -f 1 | sort | uniq -c | sort -rn\n"
      ],
      "metadata": {
        "id": "QBOV5GmMcag9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. On which date were there the most speeches about \"ilmastonmuutos\"?\n",
        "\n",
        "# Again, not cutting -f 4 keeps the date columns for use\n",
        "! cat speeches_2019_4c.csv | egrep -i 'ilmastonmuutos' | cut -d '|' -f 3 | sort | uniq -c | sort -rn"
      ],
      "metadata": {
        "id": "boPhp0xMcaeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (EXTRA) Formulate a question of your own! What would you like to find out?\n",
        "\n",
        "# What about how many speeches were given on January?\n",
        "\n",
        "# egrep -o\n",
        "# -o, --only-matching\n",
        "#     Print  only  the matched (non-empty) parts of a matching line,\n",
        "#     with each such part on a separate output line.\n",
        "\n",
        "! cat speeches_2019_4c.csv | cut -d '|' -f 3 | sort | egrep -o '2019-01' | uniq -c"
      ],
      "metadata": {
        "id": "9TH6dv70cp8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exercise 2\n",
        "\n",
        "11. Let's use the CORE-corpus available at https://github.com/TurkuNLP/CORE-corpus. Clone it to your notebook. Focus on the **development** set (dev) of the dataset. **How many documents** does it include?\n",
        "\n",
        "12. Column 1 indicates the register abbreviations of the documents. They are explained in the txt-file found in the repo. What are the **10 most frequent register combinations** found in the dataset? What actual classes do **these abbreviations correspond to**?\n",
        "\n",
        "13. Select **two registers** you want to focus on. How to make a **regular expression** that matches the register label in the first column but not the actual text?  **Make files** that consist of **frequency lists of the words** of these two registers. Before making the word frequency list, **normalize** the words to lower case and **delete numbers and punctuation**. Can you think of how to combine the delete commands to one using regexes?\n",
        "\n",
        "14. How many **unique words** do the two registers have?\n",
        "\n",
        "15. How would you search for **words that occur at least 100 times** using regex? Note that the frequency list has sometimes whitespaces in the beginning before the numbers. See how fast the frequency of the words drops!"
      ],
      "metadata": {
        "id": "_hV8Po1VAmCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Clone the repo. (Hint: Go to the website and get the repo address from the green button)\n",
        "! git clone https://github.com/TurkuNLP/CORE-corpus.git"
      ],
      "metadata": {
        "id": "-k-zafibBng3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (11. continued) Change to the right directory\n",
        "%cd CORE-corpus"
      ],
      "metadata": {
        "id": "8bYvrLFEBu5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "5OEW_PwdQjG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Focus on the development set of the dataset (dev.tsv.gz).\n",
        "! zcat dev.tsv.gz | head  # how does it look like?"
      ],
      "metadata": {
        "id": "iLgBD2GOBvrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (11. continued) How many documents does it include?\n",
        "! zcat dev.tsv.gz | wc -l # one document per line, so by counting lines we get the number of documents"
      ],
      "metadata": {
        "id": "NtXU1EePB95g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Column 1 indicates the register abbreviations of the documents.\n",
        "# They are explained in the txt-file found in the repo (register_label_abbreviations.txt).\n",
        "# What are the 10 most frequent register combinations found in the dataset?\n",
        "! zcat dev.tsv.gz | cut -f 1 | sort | uniq -ci | sort -rn | head -10 # focus on the column 1, make a frequency list"
      ],
      "metadata": {
        "id": "QBjSYgd3CF8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What actual classes do these abbreviations correspond to?\n",
        "! cat register_label_abbreviations.txt | egrep \"NA|NE|OP|OB|IN|DT|SR|ID|DF|IN|RV|IB|HI|HT\" # this will give us explanations for the labels"
      ],
      "metadata": {
        "id": "HcgLKeEvDU3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> So it would seem that Narrative + news article texts are the most frequent in the CORE corpus, followed by Opinion + opinion blogs\n",
        "\n",
        "I will compare personal blogs (pb) and travel blogs (tb), I want to understand what's the difference between these two!"
      ],
      "metadata": {
        "id": "zTr-If1GDnnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Select two registers you want to focus on.\n",
        "# How to make a regular expression that matches the register label in the first column but not the actual text?\n",
        "# Make new files that consist of frequency lists of the words of these two registers.\n",
        "# Normalize the words first to lower case and delete numbers and punctuation.\n",
        "! zcat dev.tsv.gz | egrep \"^[A-Z]+ PB\" | head  # beginning of line (^), any uppercase letter [A-Z] one or more times (+), whitespace,  PB\n"
      ],
      "metadata": {
        "id": "2Wyj8VczDkuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat dev.tsv.gz | egrep \"^[A-Z]+ PB\" | cut -f 3 > pb.txt # direct these to a file for simplicity"
      ],
      "metadata": {
        "id": "LCPlrBLdBAHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat pb.txt | tr '[:upper:]' '[:lower:]' | tr '[0-9]|[:punct:]' ' ' | tr ' ' '\\n' | head -100 # one per line and normalized"
      ],
      "metadata": {
        "id": "KNVKTubv_Vtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat pb.txt | tr '[:upper:]' '[:lower:]' | tr '[0-9]|[:punct:]' ' ' | tr ' ' '\\n' | egrep -v \"^$\" | sort | uniq -ci | sort -rn > pb-freks.txt # also empty lines away!"
      ],
      "metadata": {
        "id": "SW_zguxrAd7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's run the same pipeline now for travel blogs\n",
        "! zcat dev.tsv.gz | egrep \"^[A-Z]+ TB\" | cut -f 3 > tb.txt\n",
        "! cat tb.txt | tr '[:upper:]' '[:lower:]' | tr '[0-9]|[:punct:]' ' ' | tr ' ' '\\n' | egrep -v \"^$\" | sort | uniq -ci | sort -rn > tb-freks.txt\n"
      ],
      "metadata": {
        "id": "zfif2KPRHx_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat tb-freks.txt | head -5 # to make sure the file looks ok"
      ],
      "metadata": {
        "id": "2_XD2M-aIcE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. How many unique words do the two registers have?\n",
        "\n",
        "! cat pb-freks.txt | wc -l # how many unique words\n",
        "! cat tb-freks.txt | wc -l\n",
        "# or even\n",
        "! wc -l *freks.txt"
      ],
      "metadata": {
        "id": "249ILl9PEWay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. How many occur at least 100 times?\n",
        "! cat pb-freks.txt | egrep \"^ *[0-9][0-9][0-9] \" | wc -l # another option would be \"^ *[0-9]{3} \" where {} denote how many numbers we want"
      ],
      "metadata": {
        "id": "fltaAFlwEhoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat tb-freks.txt | head -150 | tail -50 # the beginning is so uninformative I wanted to look at the ranks 100-150"
      ],
      "metadata": {
        "id": "gPxjIn95GGCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat pb-freks.txt | head -150 | tail -50 # the beginning is so uninformative I wanted to look at the ranks 100-150"
      ],
      "metadata": {
        "id": "IP5FzQ7_4LcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 (if there is still time)\n",
        "\n",
        "Let's work on some Tweets. Unfortunately these are in Finnish, but use the file at http://dl.turkunlp.org/atp/all_tweets_one_line.txt.gz\n",
        "\n",
        "Basic stats\n",
        "* How many tweets?\n",
        "* Can you figure out what the tweets are about?\n",
        "* When have they been Tweeted?\n",
        "* Can you make a distribution of how many tweets per month the data has? The `cut` command can be useful for this. `-d` can specify the delimiter, so e.g.,  `cut -f 1,2 -d ' '` will get the first and second columns separated by a whitespace.\n",
        "\n",
        "Contents\n",
        "* Let's count the most frequent words of the tweets\n",
        "* And also the most important hashtags. And user names.\n",
        "These should tell us what we have in the data!\n",
        "\n",
        "Keywords by time and / or person?\n",
        "\n",
        "* Let's count username or time-related keywords with the python above script. How do the keywords look like, can you trace anything interesting?\n"
      ],
      "metadata": {
        "id": "RUH-e4nF_1az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget http://dl.turkunlp.org/atp/all_tweets_one_line.txt.gz"
      ],
      "metadata": {
        "id": "UvQmvCbsqW5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | head -5 #again a tweet per line!\n",
        "! zcat all_tweets_one_line.txt.gz | wc -l # this many tweets"
      ],
      "metadata": {
        "id": "pHFyOou02l9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it would seem that the column 3 has the time stamps. Let's see how the head and tail look like\n",
        "! echo \"head\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | head\n",
        "! echo \"tail\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | tail\n",
        "\n",
        "#Based on these, we could guess that the time stamps range from March 2020 to August 2021. But better yet sort the time stamps and then check the head and tail!"
      ],
      "metadata": {
        "id": "wMeBXQzW37p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! echo \"here's the head\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | sort | head\n",
        "! echo \"here's the tail\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | sort | tail\n",
        "# oups, in fact the tweets start from 2019 and continue until August 2021. We almost missed the beginning!"
      ],
      "metadata": {
        "id": "qhlhfoOW4wl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | head"
      ],
      "metadata": {
        "id": "1Hiyfkaz5QMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3 | cut  -f 1 -d ' ' | head # this gives us the dates"
      ],
      "metadata": {
        "id": "7FXSrMLV5-6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3  | cut  -f 1 -d ' ' | cut -f 1,2  -d '-' | head # and here we get just the years and months!"
      ],
      "metadata": {
        "id": "N7RK4ocZ8n98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3  | cut  -f 1 -d ' ' | cut -f 1,2  -d '-' | sort | uniq -c | sort -rn # and by making a freq. list of these, we get the distribution!\n",
        "\n",
        "# so it seems that April 2020 has clearly the most tweets, and 2019 the least"
      ],
      "metadata": {
        "id": "EsKhbzDt82UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | head # so the tweets were in column 10\n",
        "! echo \"just checking\"\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 10 | head # yeah, this one!"
      ],
      "metadata": {
        "id": "WZ01SK5p9bRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 10 | tr '[:punct:]|[0-9]' ' ' | tr ' ' '\\n' | sort | uniq -c | sort -rn | head -20 # so here's the most frequent words. quite a big data, so takes some time!"
      ],
      "metadata": {
        "id": "JDi9bobe9le4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking all punctuation was maybe not the brightest idea, it takes away the hashtags as well...\n",
        "# so let's take just the hashtags and see those\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 10 | tr ' ' '\\n' |egrep \"^#\" | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "zvk_Qb5oHZex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and usernames\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 10 | tr ' ' '\\n' |egrep \"^@\" | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "IV-3BWbkIKGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> So these are clearly (at least to some extent) related to Covid.\n",
        "--> The most frequent username is THLorg, followed by news agencies and politicians"
      ],
      "metadata": {
        "id": "SzFhFFyaIn7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat all_tweets_one_line.txt.gz | cut -f 3,10 | egrep \"^2019\" | cut -f 2 > 2019_tweets.txt\n",
        "\n",
        "! zcat all_tweets_one_line.txt.gz | cut -f 3,10 | egrep \"^2020-08\" | cut -f 2 > 2020_tweets.txt"
      ],
      "metadata": {
        "id": "daO_LmCQIamU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! head *tweets.txt # looks about right!"
      ],
      "metadata": {
        "id": "GW4O9NcEJTZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}