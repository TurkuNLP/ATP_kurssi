{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzgEisrbP7KKpmBbfvRzKw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/ATP_2025_Notebook_Python2_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using OpenAI's API"
      ],
      "metadata": {
        "id": "tCjs3JYTwA9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7LB7xLOv8DA"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "secret_token = \"\" # copy your API key token here!\n",
        "\n",
        "client = OpenAI(api_key=secret_token)\n",
        "model = \"gpt-4o-mini\"\n",
        "\n",
        "\n",
        "def estimate_cost(usage):\n",
        "  # https://platform.openai.com/docs/pricing (accessed 16.12.2025)\n",
        "  input_cost = usage.prompt_tokens / 1000000 * 0.15\n",
        "  output_cost = usage.completion_tokens / 1000000 * 0.60\n",
        "  return input_cost + output_cost\n",
        "\n",
        "def generate(prompt):\n",
        "  chat_completion = client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": prompt}], model=model)\n",
        "  cost = estimate_cost(chat_completion.usage)\n",
        "  print(\"User:\", prompt)\n",
        "  print(\"Assistant:\", chat_completion.choices[0].message.content)\n",
        "  print(f\"Cost: {cost:.6f}$\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_prompt = \"Translate into Finnish: Please write your own prompt here!\"\n",
        "generate(my_prompt)"
      ],
      "metadata": {
        "id": "Z57EMePrxqbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635f9814-21b8-42ea-cae9-bd729921c6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Translate into Finnish: Please write your own prompt here!\n",
            "Assistant: Ole hyvä ja kirjoita oma pyyntösi tähän!\n",
            "Cost: 0.000010$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing a document collection using LLMs\n",
        "\n",
        "Here is an example how to use LLMs to analyze a document collection. Here we will use few paragraphs from Project Gutenberg books, and translate those into Finnish (or any other language)."
      ],
      "metadata": {
        "id": "l3XHEcLGMb_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To avoid downloading data, I have copied here samples from three books.\n",
        "# Book 1: Frankenstein, Mary Wollstonecraft Shelley: https://www.gutenberg.org/cache/epub/84/pg84.txt (Beginning of Chapter 1)\n",
        "# Book 2: Moby Dick, Herman Melville: https://www.gutenberg.org/cache/epub/2701/pg2701.txt (Beginning of Chapter 1)\n",
        "# Book 3: Pride and Prejudice, Jane Austen: https://www.gutenberg.org/cache/epub/1342/pg1342.txt (Beginning of Chapter 1)\n",
        "\n",
        "books = [\"I am by birth a Genevese, and my family is one of the most distinguished of that republic. My ancestors had been for many years counsellors and syndics, and my father had filled several public situations with honour and reputation. He was respected by all who knew him for his integrity and indefatigable attention to public business. He passed his younger days perpetually occupied by the affairs of his country; a variety of circumstances had prevented his marrying early, nor was it until the decline of life that he became a husband and the father of a family.\",\n",
        "         \"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation.\",\n",
        "         \"It is a truth universally acknowledged, that a single man in possession of a good fortune must be in want of a wife. However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered as the rightful property of some one or other of their daughters.\"]\n",
        "\n",
        "def generate_dataset(prompt, dataset):\n",
        "  total_cost = 0\n",
        "  generated_outputs = []\n",
        "  for document in dataset:\n",
        "    prompt_with_document = prompt + document\n",
        "    chat_completion = client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": prompt_with_document}], model=model)\n",
        "    total_cost += estimate_cost(chat_completion.usage)\n",
        "    generated_outputs.append(chat_completion.choices[0].message.content)\n",
        "  print(f\"Generation done, analyzed {len(generated_outputs)} documents.\")\n",
        "  print(f\"Total cost: {total_cost:.6f}$\\n\")\n",
        "  for output in generated_outputs:\n",
        "    print(output, \"\\n\")\n",
        "\n",
        "# call the function\n",
        "my_prompt = \"Translate into Finnish: \"\n",
        "generate_dataset(my_prompt, books)\n"
      ],
      "metadata": {
        "id": "WA3NbXdwNKKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6ae05b-98e8-4f8a-d5ce-0b4af9a8ea6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation done, analyzed 3 documents.\n",
            "Total cost: 0.000268$\n",
            "\n",
            "Here is the translation into Finnish:\n",
            "\n",
            "\"Olen syntynyt geneveläinen, ja perheeni on yksi tämän tasavallan arvostetuimmista. Esivanhempani olivat olleet monta vuotta neuvonantajia ja syndikkejä, ja isäni oli täyttänyt useita julkisia tehtäviä kunnialla ja hyvässä maineessa. Häntä kunnioitettiin kaikilla, jotka tunsivat hänet, hänen integriteettinsä ja väsymätön huomionsa julkisiin asioihin vuoksi. Hän vietti nuoruutensa jatkuvasti kansansa asioiden parissa; monenlaiset olosuhteet estivät häntä menemästä naimisiin varhain, eikä hänestä tullut aviomiestä tai perheenisää ennen elämänsä ehtoopuolta.\" \n",
            "\n",
            "Sure! Here is the translation into Finnish:\n",
            "\n",
            "\"Puhelkaa minua Ishmaeliksi. Muutamia vuosia sitten—älkää kysykö kuinka pitkään—kun kukkarossani ei ollut juuri ollenkaan rahaa, enkä tiennyt mitään erityistä, mikä kiinnostaisi minua maalla, ajattelin, että purjehtisin vähän ja näkisin maailman vesisiä osia. Se on tapa, jolla karkotan alakuloa ja säädän verenkiertoa.\" \n",
            "\n",
            "Totuus on yleisesti tunnustettu, että hyvässä asemassa oleva sinkkumies on varmasti vaimon tarpeessa. Kuinka vähän tahansa tällaisen miehen tunteet tai ajatukset tunnetaan hänen ensimmäisessä saapumisessaan naapurustoon, tämä totuus on niin syvällisesti juurtunut ympäröivien perheiden mieliin, että hänet katsotaan jonkin heidän tyttärensä lailliseksi omaisuudeksi. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises\n",
        "\n",
        "1) Use `generate(\"Put your own prompt here\")` to test LLM generation. Try it with different prompts and languages. You can ask anything you like.\n",
        "\n",
        "2) Use `generate_dataset(\"Put your own prompt here\", books)` to test how an LLM can be applied to a small dataset of 3 book samples. Note that the function combines the given prompt and each book, so each time the LLM will see your instruction + one book text. Try different prompts. In addition to translation, you can try e.g. \"list all verbs\", \"convert text to uppercase\", \"who is the author of the given book\", \"simplify language\", etc.\n",
        "\n",
        "3) Extra exercise: The full book \"Pride and Prejudice\" has about 175,000 tokens (based on `gpt-4o-mini` tokenizer, note that these are different from linguistic tokens or words). Based on the pricing table, estimate what would be the cost of running the full book through `gpt-4o-mini` to e.g. convert it to uppercase? This means that the prompt would have 175,000 tokens, and the generated output would have the same 175,000 tokens. How about if using `gpt-5.2`?"
      ],
      "metadata": {
        "id": "-2TtSIVmfOxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3:\n",
        "\n",
        "# gpt-4o-mini\n",
        "input_cost = 175000 / 1000000 * 0.15\n",
        "output_cost = 175000 / 1000000 * 0.60\n",
        "print(f\"gpt-4o-mini input cost: {input_cost:.6f}$\")\n",
        "print(f\"gpt-4o-mini output cost: {output_cost:.6f}$\")\n",
        "print(f\"Total: {input_cost+output_cost:.6f}$\\n\")\n",
        "\n",
        "# gpt-5.2\n",
        "input_cost = 175000 / 1000000 * 1.75\n",
        "output_cost = 175000 / 1000000 * 14.0\n",
        "print(f\"gpt-5.2 input cost: {input_cost:.6f}$\")\n",
        "print(f\"gpt-5.2 output cost: {output_cost:.6f}$\")\n",
        "print(f\"Total: {input_cost+output_cost:.6f}$\\n\")\n",
        "\n",
        "# gpt-5.2-pro (very expensive model)\n",
        "input_cost = 175000 / 1000000 * 21.0\n",
        "output_cost = 175000 / 1000000 * 168.0\n",
        "print(f\"gpt-5.2-pro input cost: {input_cost:.6f}$\")\n",
        "print(f\"gpt-5.2-pro output cost: {output_cost:.6f}$\")\n",
        "print(f\"Total: {input_cost+output_cost:.6f}$\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSLh9aBQYgTr",
        "outputId": "eacc6662-8387-4516-eea9-0be3f40b6a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-4o-mini input cost: 0.026250$\n",
            "gpt-4o-mini output cost: 0.105000$\n",
            "Total: 0.131250$\n",
            "\n",
            "gpt-5.2 input cost: 0.306250$\n",
            "gpt-5.2 output cost: 2.450000$\n",
            "Total: 2.756250$\n",
            "\n",
            "gpt-5.2-pro input cost: 3.675000$\n",
            "gpt-5.2-pro output cost: 29.400000$\n",
            "Total: 33.075000$\n",
            "\n"
          ]
        }
      ]
    }
  ]
}