{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook8_answer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPldqU4URqD21NJMbzLkC6c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/Notebook8_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPEsOWiSpYOz"
      },
      "source": [
        "## Running python from command line\n",
        "\n",
        "```\n",
        "python3 program.py\n",
        "cat input.txt | python3 program.py\n",
        "```\n",
        "* You can find `print_sent.py` in the same folder\n",
        "`/home/mavela/dataa/syntaksijasennetyt`\n",
        "\n",
        "* This takes input from standard input so from pipe\n",
        "* As argument, you should give which column to focus\n",
        "\n",
        "`zcat suomi24.conllu.gz | python3 print_sent.py LEMMA | less`\n",
        "\n",
        "* The columns in the CoNLL format were\n",
        "\n",
        "`ID WORD LEMMA POS POS MORFOLOGY HEAD DEPREL MISC MISC`\n",
        "\n",
        "* Then the script prints sentence at a time the words and the LEMMAs as specified in the argument"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXO085OIsm-t"
      },
      "source": [
        "### Exercise on conllu datasets and treebanks\n",
        "\n",
        "* Choose a treebank from Universal dependencies website https://universaldependencies.org/#download\n",
        "* Download a treebank or at least its trainset using `git clone`\n",
        "* Let's get some basic stats. You can focus on the train file.\n",
        "**  How many words does the data have?\n",
        "**  And how many sentences?\n",
        "**  How many unique words? (normalize to lower case)\n",
        "* Remember to exclude metadata!\n",
        "---\n",
        "* Then about the lexical characteristics of the dataset\n",
        "**  How many different lemmas does the data have? How many of these appear only once?\n",
        "**  ADVANCED: you can see that the list has a lot of non-alphabet characters. Think how you could filter those away? Any difficult cases? \n",
        "** Too agressive filtering may delete useful words. Note: there's no perfect answer to this!\n",
        "**  Make a frequency list of the most frequent lemmas in the data. Make sure all are in lower case and exclude at least punctuation. What are the most frequent words? ADVANCED: any other classes you could exclude to reduce noise?\n",
        "\n",
        "--------------\n",
        "* Stop words are in NLP considered as words that don't have much content value, such as pronouns or conjunctions. This can be seen in the lemma list. \n",
        "** Let's make a frequency list which focuses on content words and excludes stop words. We could count as those nouns (NOUN), verbs (VERB), adjectives (ADJ) and adverbs (ADV).\n",
        "**  How does the lemma list look like if we include only those?\n",
        "**  Then the other way around. What are the most frequent nouns, adjectives and verbs? \n",
        "------\n",
        "*   `Get print_sent.py` and `print_some_sent.py` to your working directory, either by copying from `/home/mavela/dataa/syntaksijasennetyt`or using `git clone`.\n",
        "**  Proper nouns are tagged with PROPN. How many different proper nouns does the data have? What are the most frequent ones?\n",
        "**  `Run print_sent.py` to have a look at some sentences and their analyses\n",
        "**  Then `run print_some_sent.py` by focusing on UPOS and PROPN (proper nouns)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1IVb62L1n32"
      },
      "source": [
        "# print_sent.py takes as argument column that it will print\n",
        "# input from standard input (pipe)\n",
        "! cat en_gum-ud-train.conllu | python3 print_sent.py LEMMA | head -10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXPTgQWM19Oz"
      },
      "source": [
        "# Then run print_some_sent.py by focusing on UPOS and PROPN (proper nouns)\n",
        "! cat en_gum-ud-train.conllu | python3 print_some_sent.py UPOS PROPN "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g1vzrKIh1Tw",
        "outputId": "b2b7327c-85ef-4b5d-9a43-2c353f36e821"
      },
      "source": [
        "! git clone https://github.com/UniversalDependencies/UD_English-GUM.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UD_English-GUM'...\n",
            "remote: Enumerating objects: 3174, done.\u001b[K\n",
            "remote: Counting objects: 100% (2185/2185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (938/938), done.\u001b[K\n",
            "remote: Total 3174 (delta 1975), reused 1452 (delta 1247), pack-reused 989\u001b[K\n",
            "Receiving objects: 100% (3174/3174), 18.88 MiB | 7.94 MiB/s, done.\n",
            "Resolving deltas: 100% (2865/2865), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVJ-970Nh-V8",
        "outputId": "b1ad85aa-42bb-4c96-be0b-841a9a031640"
      },
      "source": [
        "%cd UD_English-GUM/\n",
        "! ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UD_English-GUM\n",
            "CONTRIBUTING.md        en_gum-ud-train.conllu  not-to-release\n",
            "en_gum-ud-dev.conllu   eval.log\t\t       README.md\n",
            "en_gum-ud-test.conllu  LICENSE.txt\t       stats.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMoHYi18iIy7",
        "outputId": "d71b8c30-97e8-4759-d5a0-431c013ba876"
      },
      "source": [
        "! cat en_gum-ud-train.conllu | wc -l"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha4Wl6KfiNkM"
      },
      "source": [
        "! cat en_gum-ud-train.conllu | head -30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE2D0bMhiQjB",
        "outputId": "0848f69e-6ac9-4ad1-a051-d71e5b382a2d"
      },
      "source": [
        "! cat en_gum-ud-train.conllu | egrep \"^[0-9]\" | wc -l"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mUGb_JPiqrv",
        "outputId": "1e2f5e43-c20e-4c11-e6c8-b358578d51f3"
      },
      "source": [
        "! ! cat en_gum-ud-train.conllu  | egrep \"^1[[:space:]]\" | wc -l"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0KgwcekiwAo",
        "outputId": "e6a10a06-130f-4dff-9585-0b0c27143d14"
      },
      "source": [
        " ! cat en_gum-ud-train.conllu | egrep \"^[0-9]\" | cut -f 2 | tr '[[:upper:]]' '[[:lower:]]' | sort | wc -l"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6AdDUP6lJzS"
      },
      "source": [
        "# How many different lemmas does the data have? How many of these appear only once?\n",
        "! cat en_gum-ud-train.conllu | egrep -v \"^#\" | egrep \"^[0-9]\" | egrep -v \"PUNCT|SYM|NUM\" | cut -f 3 | tr '[[:upper:]]' '[[:lower:]]' | sort | uniq | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSKDtxDZmL9Y"
      },
      "source": [
        "! cat en_gum-ud-train.conllu | egrep -v \"^#\" | egrep \"^[0-9]\" | egrep -v \"PUNCT|SYM|NUM\" | cut -f 3 | tr '[[:upper:]]' '[[:lower:]]' | sort | uniq -c | sort -n | egrep \" 1 \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COEjsxB5mYcm"
      },
      "source": [
        "# ADVANCED you can see that the list has a lot of non-alphabet characters. Think how you could filter those away? Any difficult cases? Too agressive filtering may delete useful words. Note: there's no perfect answer to this!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaabq3J6j2bp"
      },
      "source": [
        " # MAke a frequency list of the most frequent lemmas in the data. Make sure all are in lower case and exclude punctuation.\n",
        " ! cat en_gum-ud-train.conllu | egrep \"^[0-9]\" | egrep -v \"PUNCT\" | cut -f 3 | tr '[[:upper:]]' '[[:lower:]]'  | sort | uniq -c | sort -rn | head -10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZyQ5x8Uj5e5"
      },
      "source": [
        " # Stop words are in NLP considered as words that don't have much content value, such as pronouns or conjunctions. This can be seen in the lemma list. \n",
        "# Let's make a frequency list which focuses on content words. We could count as those nouns (NOUN), verbs (VERB), adjectives (ADJ) and adverbs (ADV).\n",
        "# How does the lemma list look like if we include only those?\n",
        " ! cat en_gum-ud-train.conllu | egrep \"^[0-9]\" | egrep  \"NOUN|VERB|ADJ|ADV\" | cut -f 2 | tr '[[:upper:]]' '[[:lower:]]'  | sort | uniq -c | sort -rn | head -20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXsk48q6xR78"
      },
      "source": [
        "# Then the other way around. What are the most frequent nouns and verbs?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW2fEJv-kBQp"
      },
      "source": [
        "# Proper nouns are tagged with PROPN. How many different proper nouns does the data have? What are the most frequent ones?\n",
        "! cat en_gum-ud-train.conllu | egrep \"PROPN\"| cut -f 3 | sort | uniq -c | sort -rn | head -10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lIPV6_hnBdp"
      },
      "source": [
        "# Get print_sent.py and print_some_sent.py to your working directory, either by copying from /home/mavela/dataa/syntaksijasennetyt\n",
        " # or using git clone.\n",
        " # Run print_sent.py to have a look at some sentences and their analyses\n",
        "! cat en_gum-ud-train.conllu | python3 print_sent.py LEMMA | head -10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c14a0KTmuWlD"
      },
      "source": [
        "# Then run print_some_sent.py by focusing on UPOS and PROPN (proper nouns)\n",
        "! cat en_gum-ud-train.conllu | python3 print_some_sent.py UPOS PROPN | "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JquK_n53tfN9",
        "outputId": "92af76df-e89a-445c-b11a-dd2e5872b601"
      },
      "source": [
        "! git clone https://github.com/TurkuNLP/ATP_kurssi.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ATP_kurssi'...\n",
            "remote: Enumerating objects: 442, done.\u001b[K\n",
            "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
            "remote: Compressing objects: 100% (198/198), done.\u001b[K\n",
            "remote: Total 442 (delta 111), reused 33 (delta 10), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (442/442), 20.14 MiB | 16.78 MiB/s, done.\n",
            "Resolving deltas: 100% (240/240), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VJeRMiUtq81"
      },
      "source": [
        "! cp ATP_kurssi/print* ."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeQRTqofuEYf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}