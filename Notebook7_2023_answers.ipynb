{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/Notebook7_2022_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "The course Github repo has the tweets syntax analyzed in the data folder. The file is called simply `covidtweets.conllu.gz `. Let's work on this a bit.\n",
        "\n",
        "### Basics\n",
        "* How many tweets?\n",
        "* How many tokens? What if you exclude punctuation and numbers? \n",
        "* How many sentences? Note that `\\t` does not work with egrep, can you google for how to do this?\n",
        "\n",
        "### Lexical characteristics\n",
        "* The most frequent lemmas? What if you exclude function words? \n",
        "* The definition of function words can vary a bit. What do you think could be the most useful POS classes to keep to get a general view to the contents of the tweets?\n",
        "* Note: it might be hard to figure out the POS tags associated with the words, you can also analyze this by combining the lemma and POS columns\n",
        "\n",
        "### More\n",
        "* Now that we have POS classes, we can also focus on specific kinds of words. So let's count the most frequent lemmas for\n",
        "  * Nouns (NOUN)\n",
        "  * Verbs (VERB)\n",
        "  * Proper nouns (PROPR)\n",
        "* Which POS class words provide in your opinion the most interesting results?"
      ],
      "metadata": {
        "id": "xx16JAjMekGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/TurkuNLP/ATP_kurssi.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gnhi-wzxyoW",
        "outputId": "461b9cbf-d84e-4f9f-a641-cec790538ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ATP_kurssi'...\n",
            "remote: Enumerating objects: 596, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 596 (delta 0), reused 0 (delta 0), pack-reused 594\u001b[K\n",
            "Receiving objects: 100% (596/596), 79.58 MiB | 25.94 MiB/s, done.\n",
            "Resolving deltas: 100% (336/336), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ATP_kurssi/data"
      ],
      "metadata": {
        "id": "IvI-K5jzeEJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| head # we can see that each tweet starts with the mention ###C: NEWDOC\n"
      ],
      "metadata": {
        "id": "V85_1LiJeI-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^###C: NEWDOC\" | wc -l # we can just grep for those and count the lines"
      ],
      "metadata": {
        "id": "oSQ_h_ZphIEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31b4ec3-010f-4933-8763-b36d7dbb6a8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: covidtweets.conllu.gz: No such file or directory\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for tokens, we need to focus on the lines starting with a number and count those\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | head #these seem to match the correct lines"
      ],
      "metadata": {
        "id": "D7UwWbDBhS2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | wc -l"
      ],
      "metadata": {
        "id": "fvgQ5Fv_hcRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to exclude tokens tagged as numbers or punctuation, we should exclude lines w those tags\n",
        "# first we need to figure out those tags. \n",
        "# this can be searched for too! \n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"!\" | head "
      ],
      "metadata": {
        "id": "Re8aeKZ2h13C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to find how numbers are tagged, I'll keep the columns 2 (for the running words) and 4 (POS)\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | cut -f 2,4 | egrep \"[0-9]\" | head "
      ],
      "metadata": {
        "id": "qV_Bh-3kiMId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then the tokens wo punctuation and numbers\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"NUM|PUNCT\" | head # looks about right!"
      ],
      "metadata": {
        "id": "xxhfL2_kjiVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"NUM|PUNCT\" | wc -l"
      ],
      "metadata": {
        "id": "5Lu84Wd8jr16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to count sentences, we can search for sentences starting with 1.\n",
        "# be sure to match just 1, not 10!\n",
        "# [[:space:]] works with egrep and matches \\t\n",
        "# another option is grep -P, which accepts \\t\n",
        "# surely there can be others as well!\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^1[[:space:]]\" | head -5\n",
        "! zcat covidtweets.conllu.gz| grep -P \"^1\\t\" | head -5"
      ],
      "metadata": {
        "id": "a_QPNIp4j8ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| grep -P \"^1\\t\" | wc -l # then the sentence count!"
      ],
      "metadata": {
        "id": "Q5kaSjQglniW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Then the lexical characteristics, let's start with the lemmas"
      ],
      "metadata": {
        "id": "sHzz4dkVl0jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmas are column number 3\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | cut -f 3 | head"
      ],
      "metadata": {
        "id": "sEGVCFHFl5PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can just do the frequency list from column 3\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | cut -f 3 | sort | uniq -c | sort -rn | head"
      ],
      "metadata": {
        "id": "IfR7xxR9mcG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to exclude at least numbers and punctuation for starters\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM\" | cut -f 3 | sort | uniq -c | sort -rn | head"
      ],
      "metadata": {
        "id": "XxEqjXkhmnwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there's a lot I dont want to have in the above list. But it's kinda hard to know what POS categories they are, so I'll just do a \n",
        "# frequency list with the lemmas + POS tags"
      ],
      "metadata": {
        "id": "ibKulcXVm_A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM\" | cut -f 3,4 | sort | uniq -c | sort -rn | head"
      ],
      "metadata": {
        "id": "hPl_R3kbnGAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so at least SCONJ, AUX, PRON away..."
      ],
      "metadata": {
        "id": "jbIDsqmMnp-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM|PRON|SCONJ|AUX\"  | cut -f 3,4 | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "bt2GP29Un7nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looks so much better! but yet at least CCONJ away...\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM|PRON|SCONJ|CCONJ|AUX\"  | cut -f 3,4 | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "PcM1DRBNoLcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but I think that one does it!"
      ],
      "metadata": {
        "id": "P87aBl-soZyM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then the specific POS classes"
      ],
      "metadata": {
        "id": "CnsArWIzp5nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"NOUN\" | head # so these match nouns\n",
        "# then just column 3 and the frequencies"
      ],
      "metadata": {
        "id": "FBr8O1gVp9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"NOUN\" | cut -f 3 | sort | uniq -c | sort -rn| head -20"
      ],
      "metadata": {
        "id": "krPW1hZ6qIcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"ADJ\" | cut -f 3 | sort | uniq -c | sort -rn| head -20 # quite sick adjectives right?"
      ],
      "metadata": {
        "id": "2lHdn5oBqWj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"VERB\" | cut -f 3 | sort | uniq -c | sort -rn| head -20"
      ],
      "metadata": {
        "id": "pGeuPnRsqem-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Exercise 2\n",
        "\n",
        "With a python wrapper we can print only the tweets that match the content and time of our query. \n",
        "\n",
        "Here's an example:\n",
        "\n",
        "`! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"Covid\" --time \"2020\" `\n",
        "\n",
        "Both options match any string in the respective metadatalines, so `--time \"2021\"` and `\"2021-02\" `are both valid options.\n",
        "\n",
        "Also, the --text option normalises everything to lower case, `so --text \"TWEET\"` matches any upper / lower case versions.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Let's focus on tweets that mention specific persons. At least the prime minister @MarinSanna is mentioned frequently, so let's try her. How could you first fetch just the tweets that mention her? Direct those tweets to a file.\n",
        "\n",
        "How many tweets does this file have? How are they distributed over time? What would be a reasonable way to analyze the distribution - the time stamps are quite detailed and not equally distributed? \n",
        "\n",
        "Let's try to compare the contents of tweets mentioning @MarinSanna at different periods of time. Ideally, we would have a sufficient number of tweets to compare, representing different time spots of the crisis.\n",
        "\n",
        "Gather the tweets to be compared, and analyze possible differences in terms of frequent words. Which POS classes provide the most interpretable results? If any? Can you see some differences that could be related to the different periods in the crisis and events that took place?\n",
        "\n",
        "When you find interesting words that could reflect some events, remember to analyze tweets with those words to check if the words are used like you anticipated.\n",
        "\n",
        "If you have time left after this, you can try with different politicians or other handles. For instance, @THL_org seems quite frequent - what is it and what do twitters tweet about it?\n"
      ],
      "metadata": {
        "id": "HqM6zOsRq4jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"Covid\" --time \"2020\" | head -40"
      ],
      "metadata": {
        "id": "607-U-SPH9Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2019-12\" --text \"pää\" | head -40"
      ],
      "metadata": {
        "id": "jfXp14NjOfCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then the tweets with @MarinSanna to a file\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"@MarinSanna\" | gzip > sannamarin.conllu.gz"
      ],
      "metadata": {
        "id": "JMMXw8bySgk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat sannamarin.conllu.gz | head -50 # these look ok!"
      ],
      "metadata": {
        "id": "Cu2onssDTIA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# by grepping and counting the lines starting with ###C:TEXT we can count the number of tweets\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TEXT\" | wc -l "
      ],
      "metadata": {
        "id": "yFVdGxjSTYea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then by grepping and counting the time stamps we can get their distribution over time\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" | sort | uniq -c | sort -rn | head \n",
        "# but clearly these stamps are too detailed, no trends can be seen"
      ],
      "metadata": {
        "id": "CHyCLR1OTvEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I'll take months (and delete the days and times)\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" |  cut -f 2 -d ' '| head  # for the dates"
      ],
      "metadata": {
        "id": "g7Wno0mlUguX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the months\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" |  cut -f 2 -d ' '| cut -f 1,2 -d '-' | head # this looks ok!"
      ],
      "metadata": {
        "id": "yvCE-KNfVETr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to make sure, let's see how the distribution is\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" |  cut -f 2 -d ' '| cut -f 1,2 -d '-' | sort | uniq -c | sort -rn | head -30\n",
        "\n",
        "# or, actually, I think the distribution is kinda wonky, mostly just from the first months of the crisis. "
      ],
      "metadata": {
        "id": "aPyQt-vbVNSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what about the first days of the crisis? What would be frequent enough?\n",
        "# Based on this frequency list, I'll take March 17 and March 18 \n",
        "! zcat sannamarin.conllu.gz | egrep \"TIME\" | egrep \"2020-03\" | cut -f 2 -d ' ' | cut -f 1,2,3 -d '-'| sort | uniq -c | sort -rn | head -30"
      ],
      "metadata": {
        "id": "TwrG2N-BjanK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unfortunately my python script doesn't support regexes\n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-03-18\" > 03-18.conllu \n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-03-17\" > 03-17.conllu "
      ],
      "metadata": {
        "id": "bTEHvDd7lDxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to check the numbers match - looks like ok!\n",
        "! cat 03-18.conllu 03-17.conllu | egrep \"###C: NEWDOC\" | wc -l\n",
        "! cat 03-18.conllu 03-17.conllu | egrep \"###C:TIME\" | cut -f 2 -d ' ' | cut -f 1,2,3 -d '-' | sort | uniq -c | sort -rn"
      ],
      "metadata": {
        "id": "jx_cpDpplfn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the most frequent adjectives, nouns, verbs\n",
        "! cat 03-18.conllu 03-17.conllu  | egrep \"ADJ|NOUN|VERB\" | cut -f 3 | sort | uniq -c | sort -rn | head -20\n",
        "# I guess these are ok, but I'm not sure about the verbs. would the list be better without verbs?"
      ],
      "metadata": {
        "id": "vclBja4imt6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat 03-18.conllu 03-17.conllu  | egrep \"ADJ|NOUN\" | cut -f 3 | sort | uniq -c | sort -rn | head -20\n"
      ],
      "metadata": {
        "id": "wQJ4BfClocjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# well, I guess these do reflect some moments in the crisis! \n",
        "# there's valmiuslaki \"emergency powers legislation\", tärkeä \"important\", raja \"border\"\n",
        "# I'll yet check how these are used in the tweets. Unf. my script doesn't search for lemmas, but maybe the string will get some matches anyway!\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu  | python3 ../scripts/read_conllu_docs.py --text \"raja\" | egrep \"###C:TEXT\" | head\n",
        "# well, not horrible! The tweets mostly seem to discuss borders and border control"
      ],
      "metadata": {
        "id": "yMcGivZFohZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the comparison to April. Again, we need enough tweets, preferably from late April so there's some time gap to the March tweets we have. Let's see how many tweets we have from late April"
      ],
      "metadata": {
        "id": "kMHZAl_9pkoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# these are now just the days of April\n",
        "! zcat sannamarin.conllu.gz | egrep \"###C:TIME\" | cut -f 2 -d ' ' | egrep \"2020-04\" | cut -f 3 -d '-' | sort | uniq -c | sort -rn \n",
        "\n",
        "# not many tweets from late April, but let's take April 15 and 14"
      ],
      "metadata": {
        "id": "kumNSsIDr6ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-04-15\" >  04-15.conllu \n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-04-14\" >  04-14.conllu"
      ],
      "metadata": {
        "id": "nMI52epLXygc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat 04*  | egrep \"ADJ|NOUN\" | cut -f 3 | sort | uniq -c | sort -rn | head -20 "
      ],
      "metadata": {
        "id": "50vi0hlolcez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I guess there's some differences, at least mökki \"summer house\" and maski \"mask\". Let's see how these tweets with these words look like. "
      ],
      "metadata": {
        "id": "L-KkpLE1uOUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cat 04* | python3 ../scripts/read_conllu_docs.py --text \"mökki\" | egrep \"###C:TEXT\" | head\n",
        "\n",
        "# well, looks like I should I'd think!"
      ],
      "metadata": {
        "id": "V2z2gN6_umfl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
