{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/ATP_kurssi/blob/master/2024_Notebook_7_answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise I\n",
        "\n",
        "In the Github repo https://github.com/TurkuNLP/ATP_kurssi.git there are syntax analyzed tweets in a folder called 'data'. The file is called simply `covidtweets.conllu.gz `. Let's work on this a bit.\n",
        "\n",
        "### 1. Preparations\n",
        "\n",
        "\n",
        "*   a. clone the repo\n",
        "*   b. go to the folder\n",
        "\n",
        "\n",
        "### 2. Basics\n",
        "* a. How many tweets?\n",
        "* b. How many tokens? What if you exclude punctuation and numbers?\n",
        "* c. How many sentences? Note that `\\t` (tab which separates the columns in the file) does not work with egrep, can you google for how to do this?\n",
        "\n",
        "### 3. Lexical characteristics\n",
        "* a. The most frequent lemmas?\n",
        "* b. What if you exclude function words? The definition of function words can vary a bit. What do you think could be the most useful POS classes to keep to get a general view to the contents of the tweets?\n",
        "* Note: it might be hard to figure out the POS tags associated with the words, you can also analyze this by combining the lemma and POS columns\n",
        "\n",
        "### 4. More\n",
        "* Now that we have POS classes, we can also focus on specific kinds of words. So let's count the most frequent lemmas for\n",
        "  * a. nouns (NOUN)\n",
        "  * b. adjectives (ADJ)\n",
        "  * c. verbs (VERB)\n",
        "* Which POS class words provide the most interesting results in your opinion?"
      ],
      "metadata": {
        "id": "xx16JAjMekGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1a. Let's start by cloning the repo\n",
        "\n",
        "! git clone https://github.com/TurkuNLP/ATP_kurssi.git"
      ],
      "metadata": {
        "id": "0gnhi-wzxyoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1b. Then go to the folder where the covidtweets file is\n",
        "\n",
        "%cd ATP_kurssi/data"
      ],
      "metadata": {
        "id": "IvI-K5jzeEJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#As always, let's start by checking the file\n",
        "\n",
        "#remember to print the gzipped file with zcat\n",
        "\n",
        "! zcat covidtweets.conllu.gz| head # we can see that each tweet starts with the mention ###C: NEWDOC\n"
      ],
      "metadata": {
        "id": "V85_1LiJeI-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2a. Count the tweets\n",
        "# we can just grep for ###C: NEWDOC indicating the start of a new document and count the lines\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^###C: NEWDOC\" | wc -l"
      ],
      "metadata": {
        "id": "oSQ_h_ZphIEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2b. Count the tokens\n",
        "# for tokens, we need to focus on the lines starting with a number and count those\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | head #these seem to match the correct lines"
      ],
      "metadata": {
        "id": "D7UwWbDBhS2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the lines for token count\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | wc -l"
      ],
      "metadata": {
        "id": "fvgQ5Fv_hcRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586814c6-7796-433c-d2ae-db77fba36828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4512680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to exclude tokens tagged as numbers or punctuation, we should exclude lines with those tags\n",
        "# first we need to figure out those tags.\n",
        "# this can be searched for too!\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"!\" | head"
      ],
      "metadata": {
        "id": "Re8aeKZ2h13C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N.B.** The fourth column gives the POS tag for a token, and here we see that punctuation marks are tagged with PUNCT. We can then `egrep -v \"PUNCT\"` to exclude punctuation."
      ],
      "metadata": {
        "id": "dmVtkPGJW8gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#similarly, we can find the POS tag for numbers\n",
        "#to make it a bit simpler (and easier to read), I'll keep the columns 2 (for the running words) and 4 (POS)\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | cut -f 2,4 | egrep \"[0-9]\" | head"
      ],
      "metadata": {
        "id": "qV_Bh-3kiMId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that the POS tag for numbers is NUM. We can `egrep -v \"NUM|PUNCT\"` to remove both numbers and punctuation simultaneously.\n",
        "\n",
        "Remember that `|`is alternative, and thus `egrep -v \"NUM|PUNCT\"`matches lines that don't include NUM or PUNCT."
      ],
      "metadata": {
        "id": "YguCD_8OXrqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, get the tokens without punctuation and numbers\n",
        "\n",
        "#grep the lines that start with a number (get the tokens)\n",
        "#remove the lines with numbers or punctuation\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"NUM|PUNCT\" | head -50 # looks about right!"
      ],
      "metadata": {
        "id": "xxhfL2_kjiVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check if the numbers were removed by comparing with the original file\n",
        "#and we can see that the number 18.5 is removed indeed\n",
        "#however, 7/x # remains since it is not tagged as NUM, but SYM\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | head -50"
      ],
      "metadata": {
        "id": "ui1VQy4FZ-9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count the tokens without numbers and punctuation\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"NUM|PUNCT\" | wc -l"
      ],
      "metadata": {
        "id": "5Lu84Wd8jr16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2c.Count the sentences\n",
        "# to count sentences, we can search for lines with 1 (the first token in each sentence has line number 1)\n",
        "# be sure to match just 1, not 10!\n",
        "# [[:space:]] works with egrep and matches \\t\n",
        "# another option is grep -P, which accepts \\t\n",
        "# surely there can be others as well!\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^1[[:space:]]\" | head -5\n",
        "! zcat covidtweets.conllu.gz| grep -P \"^1\\t\" | head -5"
      ],
      "metadata": {
        "id": "a_QPNIp4j8ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then the sentence count!\n",
        "\n",
        "! zcat covidtweets.conllu.gz| grep -P \"^1\\t\" | wc -l"
      ],
      "metadata": {
        "id": "Q5kaSjQglniW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Then the lexical characteristics, let's start with the lemmas"
      ],
      "metadata": {
        "id": "sHzz4dkVl0jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3a. The most frequent lemma\n",
        "#lemmas are in column number 3\n",
        "\n",
        "#grep the token lines\n",
        "#choose the correct column\n",
        "#and check that you got the lemma column\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | cut -f 3 | head"
      ],
      "metadata": {
        "id": "sEGVCFHFl5PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can just do the frequency list from column 3\n",
        "\n",
        "#after cut -f 3 add the frequency list\n",
        "#and check what the frequency list looks like\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | cut -f 3 | sort | uniq -c | sort -rn | head"
      ],
      "metadata": {
        "id": "IfR7xxR9mcG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3b. Remove function words\n",
        "# let's start with removing numbers and punctuation\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM\" | cut -f 3 | sort | uniq -c | sort -rn | head"
      ],
      "metadata": {
        "id": "XxEqjXkhmnwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there's a lot I don't want to have in the above list.\n",
        "#But it's kinda hard to know what POS categories they are, so first,\n",
        "# I'll just do a frequency list with the lemmas + POS tags"
      ],
      "metadata": {
        "id": "ibKulcXVm_A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#grep the token lines\n",
        "#remove punctuation and numbers\n",
        "#take the lemma and pos columns\n",
        "#(N.B. if you use cut -f on several columns, there's no space between the comma and the following number)\n",
        "#and check that you got the correct columns\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM\" | cut -f 3,4 | head"
      ],
      "metadata": {
        "id": "hPl_R3kbnGAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#then do the frequency list of the lemmas and pos tags\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM\" | cut -f 3,4 | sort | uniq -c | sort -rn | head"
      ],
      "metadata": {
        "id": "WvoEP1BwdrTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3b.\n",
        "# so at least SCONJ, AUX, PRON away..."
      ],
      "metadata": {
        "id": "jbIDsqmMnp-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM|PRON|SCONJ|AUX\"  | cut -f 3,4 | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "bt2GP29Un7nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looks so much better! but yet at least CCONJ away...\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep -v \"PUNCT|NUM|PRON|SCONJ|CCONJ|AUX\"  | cut -f 3,4 | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "PcM1DRBNoLcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but I think that one does it!"
      ],
      "metadata": {
        "id": "P87aBl-soZyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then the specific POS classes"
      ],
      "metadata": {
        "id": "CnsArWIzp5nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4a. Most frequent lemmas for NOUN\n",
        "\n",
        "#grep the NOUNS\n",
        "#check that you got the NOUNS\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"NOUN\" | head # so these match nouns"
      ],
      "metadata": {
        "id": "FBr8O1gVp9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then just column 3 and the frequencies\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"NOUN\" | cut -f 3 | sort | uniq -c | sort -rn| head -20"
      ],
      "metadata": {
        "id": "krPW1hZ6qIcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4b. Most frequent lemmas for ADJ\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"ADJ\" | cut -f 3 | sort | uniq -c | sort -rn| head -20"
      ],
      "metadata": {
        "id": "2lHdn5oBqWj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4c. Most frequent lemmas for VERB\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"VERB\" | cut -f 3 | sort | uniq -c | sort -rn| head -20"
      ],
      "metadata": {
        "id": "pGeuPnRsqem-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXTRA TIME LEFT?**\n",
        "\n",
        "Choose a word and count the frequencies of the different forms of the word."
      ],
      "metadata": {
        "id": "MgnpZTm9h-b5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I decided to have a look at the different adjective compounds with the word 'korona'\n",
        "\n",
        "! zcat covidtweets.conllu.gz| egrep \"^[0-9]\" | egrep \"korona#\" | cut -f 3,4 | egrep \"ADJ\" | sort | uniq -c | sort -nr | head"
      ],
      "metadata": {
        "id": "jlgD1hd14HUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Exercise II\n",
        "\n",
        "In this exercise we'll use a **Python wrapper**. Let's not go into detail here, since this is not a course in Python, instead it suffices to understand this on the surface.\n",
        "\n",
        "The point is to show how we can use Python scripts in Bash to get more out of our data. In this Python wrapper we use a script that takes **arguments**, and in these arguments we can **specify details** of the Tweets we want to find.\n",
        "\n",
        "With this Python wrapper we can print only the tweets that match the content and time of our query.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "`! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"Covid\" --time \"2020\" `\n",
        "\n",
        "Let's break this down.\n",
        "\n",
        "\n",
        "\n",
        "*   First we print the file `! zcat covidtweets.conllu.gz`\n",
        "*   Then on this file we run the Python script. We need to give the command `python3` and the path to the script `../scripts/read_conllu_docs.py`\n",
        "*   Finally we have two options marked with `--`, in this case `--time` and `--text`\n",
        "\n",
        "\n",
        "  *   These refer to the metadatalines in the data file\n",
        "\n",
        "      `###C:TIME`\n",
        "      \n",
        "      `###C:TEXT`\n",
        "\n",
        "\n",
        "\n",
        "  *   By assigning a specific time or word(s) to these options respectively, we can get Tweets from a specific date/time frame and on a specific topic\n",
        "\n",
        "\n",
        "\n",
        "  *   Both options match any string in the respective metadatalines, so all of the following are valid options:\n",
        "  \n",
        "      `--time \"2021\"`  \n",
        "      `--time \"2021-02\" `\n",
        "\n",
        "      `--text \"Covid\"`  \n",
        "      `--text \"korona on\" `\n",
        "\n",
        "\n",
        "  *   Also, the `--text` option normalises everything to lower case, `so --text \"TWEET\"` matches any upper / lower case versions.\n",
        "\n",
        "Let's see how the Python wrapper works. In the example, we look for tweets with a mention of \"Covid\" from the year 2020."
      ],
      "metadata": {
        "id": "HqM6zOsRq4jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tweets with \"Covid\" from 2020.\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"Covid\" --time \"2020\" | head -40"
      ],
      "metadata": {
        "id": "607-U-SPH9Y4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c00277-58d9-42a5-a4a6-65e6c7f72993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###C: NEWDOC\n",
            "###C:TIME 2020-03-30 23:58:37\n",
            "###C:TEXT The Lancetissa julkaistu tutkimus Kiinan aineistosta: potilaat olivat keskimäärin 18.5 päivää sairaana ennen kuolemaa.  7/x  #koronavirusfi  #korona  #covid19fi  #covid19finland  #covid2019  https://t.co/VKzxCtG8C0\n",
            "\n",
            "\n",
            "1\tThe\tThe\tPROPN\tN\tCase=Nom|Number=Sing\t3\tobl\t_\t_\n",
            "2\tLancetissa\tLancet\tPROPN\tN\tCase=Ine|Number=Sing\t1\tflat:name\t_\t_\n",
            "3\tjulkaistu\tjulkaista\tVERB\tV\tCase=Nom|Degree=Pos|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Pass\t4\tacl\t_\t_\n",
            "4\ttutkimus\ttutkimus\tNOUN\tN\tCase=Nom|Number=Sing\t0\troot\t_\t_\n",
            "5\tKiinan\tKiina\tPROPN\tN\tCase=Gen|Number=Sing\t6\tnmod:poss\t_\t_\n",
            "6\taineistosta\taineisto\tNOUN\tN\tCase=Ela|Number=Sing\t4\tnmod\t_\t_\n",
            "7\t:\t:\tPUNCT\tPunct\t_\t13\tpunct\t_\t_\n",
            "8\tpotilaat\tpotilas\tNOUN\tN\tCase=Nom|Number=Plur\t13\tnsubj:cop\t_\t_\n",
            "9\tolivat\tolla\tAUX\tV\tMood=Ind|Number=Plur|Person=3|Tense=Past|VerbForm=Fin|Voice=Act\t13\tcop\t_\t_\n",
            "10\tkeskimäärin\tkeski#määrin\tADV\tAdv\t_\t11\tadvmod\t_\t_\n",
            "11\t18.5\t18.5\tNUM\tNum\tNumType=Card\t12\tnummod\t_\t_\n",
            "12\tpäivää\tpäivä\tNOUN\tN\tCase=Par|Number=Sing\t13\tnmod\t_\t_\n",
            "13\tsairaana\tsairas\tADJ\tA\tCase=Ess|Degree=Pos|Number=Sing\t4\tnmod\t_\t_\n",
            "14\tennen\tennen\tADP\tAdp\tAdpType=Prep\t15\tcase\t_\t_\n",
            "15\tkuolemaa\tkuolema\tNOUN\tN\tCase=Par|Number=Sing\t13\tnmod\t_\t_\n",
            "16\t.\t.\tPUNCT\tPunct\t_\t4\tpunct\t_\t_\n",
            "\n",
            "\n",
            "1\t7/x  #\t7/x  #\tSYM\tSymb\t_\t0\troot\t_\t_\n",
            "2\tkoronavirusfi  #\tkoronavirusfi  #\tSYM\tSymb\tCase=Nom|Number=Sing\t1\tconj\t_\t_\n",
            "3\tkorona\tkoro\tNOUN\tN\tCase=Nom|Number=Sing\t1\tconj\t_\t_\n",
            "4\t#\t#\tPUNCT\tPunct\t_\t5\tpunct\t_\t_\n",
            "5\tcovid19fi  #\tcovid19fi  #\tSYM\tSymb\t_\t1\tconj\t_\t_\n",
            "6\tcovid19finland  #\tcovid19finland  #\tSYM\tSymb\t_\t1\tconj\t_\t_\n",
            "7\tcovid2019  https://t.co/VKzxCtG8C0\tcovid2019  https://t.co/VKzxCtG8C0\tSYM\tSymb\t_\t1\tappos\t_\t_\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "###C: NEWDOC\n",
            "###C:TIME 2020-03-30 23:50:42\n",
            "###C:TEXT Kuvan tulkinta:  -   ensin pitää tehdä oletus siitä, montako päivää kului tartunnan saamisesta kuolemaan -   esim. 30.3. yhteensä 13 kuollutta, 15 pv aikaisemmin (15.3.) yht 244 tartuntaa, 13/224=5.3% 5/x  #koronavirusfi  #korona  #covid19fi  #covid19finland  #covid2019\n",
            "\n",
            "\n",
            "1\tKuvan\tkuva\tNOUN\tN\tCase=Gen|Number=Sing\t2\tnmod:gobj\t_\t_\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ATP_kurssi/data/ATP_kurssi/data/../scripts/read_conllu_docs.py\", line 70, in <module>\n",
            "    print_all(time,text,feats)\n",
            "  File \"/content/ATP_kurssi/data/ATP_kurssi/data/../scripts/read_conllu_docs.py\", line 57, in print_all\n",
            "    print(\"\\t\".join(f))\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we can see that the options match any string\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2019-12\" --text \"aurinko paistaa\" | head -40"
      ],
      "metadata": {
        "id": "jfXp14NjOfCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7354b7-3b00-4471-dc0c-d26deb758c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###C: NEWDOC\n",
            "###C:TIME 2019-12-03 09:20:25\n",
            "###C:TEXT Ollut ”vähän väsynyt” kun vauva valvottanut öisin jo viikkoja. Onneksi aurinko paistaa! Helpottaa. Olotilasta kertoo se, että vaikka päivittäin lukee uutisia niin aivan pihalla tän hetkisestä meiningistä. Selkeästi mikään luettu ei ole mennyt perille. #univaje #vauvavuosi\n",
            "\n",
            "\n",
            "1\tOllut\tolla\tAUX\tV\tCase=Nom|Degree=Pos|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Act\t4\tcop\t_\t_\n",
            "2\t”\t”\tPUNCT\tPunct\t_\t4\tpunct\t_\t_\n",
            "3\tvähän\tvähän\tADV\tAdv\t_\t4\tadvmod\t_\t_\n",
            "4\tväsynyt\tväsynyt\tADJ\tA\tCase=Nom|Degree=Pos|Number=Sing\t0\troot\t_\t_\n",
            "5\t”\t”\tPUNCT\tPunct\t_\t4\tpunct\t_\t_\n",
            "6\tkun\tkun\tSCONJ\tC\t_\t8\tmark\t_\t_\n",
            "7\tvauva\tvauva\tNOUN\tN\tCase=Nom|Number=Sing\t8\tnsubj\t_\t_\n",
            "8\tvalvottanut\tvalvottaa\tVERB\tV\tCase=Nom|Degree=Pos|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Act\t4\tadvcl\t_\t_\n",
            "9\töisin\töisin\tADV\tAdv\t_\t8\tadvmod\t_\t_\n",
            "10\tjo\tjo\tADV\tAdv\t_\t11\tadvmod\t_\t_\n",
            "11\tviikkoja\tviikko\tNOUN\tN\tCase=Par|Number=Plur\t8\tobl\t_\t_\n",
            "12\t.\t.\tPUNCT\tPunct\t_\t4\tpunct\t_\t_\n",
            "\n",
            "\n",
            "1\tOnneksi\tonneksi\tADV\tAdv\t_\t3\tadvmod\t_\t_\n",
            "2\taurinko\taurinko\tNOUN\tN\tCase=Nom|Number=Sing\t3\tnsubj\t_\t_\n",
            "3\tpaistaa\tpaistaa\tVERB\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "4\t!\t!\tPUNCT\tPunct\t_\t3\tpunct\t_\t_\n",
            "\n",
            "\n",
            "1\tHelpottaa\thelpottaa\tVERB\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "2\t.\t.\tPUNCT\tPunct\t_\t1\tpunct\t_\t_\n",
            "\n",
            "\n",
            "1\tOlotilasta\tolo#tila\tNOUN\tN\tCase=Ela|Number=Sing\t2\tobl\t_\t_\n",
            "2\tkertoo\tkertoa\tVERB\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
            "3\tse\tse\tPRON\tPron\tCase=Nom|Number=Sing|PronType=Dem\t2\tnsubj\t_\t_\n",
            "4\t,\t,\tPUNCT\tPunct\t_\t12\tpunct\t_\t_\n",
            "5\tettä\tettä\tSCONJ\tC\t_\t12\tmark\t_\t_\n",
            "6\tvaikka\tvaikka\tSCONJ\tC\t_\t8\tmark\t_\t_\n",
            "7\tpäivittäin\tpäivittäin\tADV\tAdv\tDerivation=Ttain\t8\tadvmod\t_\t_\n",
            "8\tlukee\tlukea\tVERB\tV\tMood=Ind|Number=Sing|Person=0|Tense=Pres|VerbForm=Fin|Voice=Act\t12\tadvcl\t_\t_\n",
            "9\tuutisia\tuutinen\tNOUN\tN\tCase=Par|Number=Plur\t8\tobj\t_\t_\n",
            "10\tniin\tniin\tADV\tAdv\t_\t12\tadvmod\t_\t_\n",
            "11\taivan\taivan\tADV\tAdv\t_\t12\tadvmod\t_\t_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! zcat covidtweets.conllu.gz | egrep \"###C:TIME\" | cut -f 2 -d ' ' | cut -f 1 -d '-' | sort | uniq -c | sort -nr | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ICFQQCBPmPD",
        "outputId": "68bd00f1-6b5f-4162-8289-a9e92909cf21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 165026 2020\n",
            "   5837 2019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercises**\n",
        "\n",
        "\n",
        "Let's focus on tweets that mention specific persons. At least the prime minister @MarinSanna is mentioned frequently, so let's try her.\n",
        "\n",
        "1. How could you first fetch just the tweets that mention her? Direct those tweets to a file.\n",
        "\n",
        "2. a) How many tweets does this file have? How are they distributed over time?\n",
        "b) What would be a reasonable way to analyze the distribution - the time stamps are quite detailed and not equally distributed?\n",
        "\n",
        "3. Let's try to compare the contents of tweets mentioning @MarinSanna at different periods of time. Ideally, we would have a sufficient number of tweets to compare, representing different time spots of the crisis.\n",
        "\n",
        "  a) Gather the tweets to be compared, and analyze possible differences in terms of frequent words. Which POS classes provide the most interpretable results? If any?\n",
        "  \n",
        "  b) Can you see some differences that could be related to the different periods in the crisis and events that took place?\n",
        "\n",
        "  When you find interesting words that could reflect some events, remember to analyze tweets with those words to check if the words are used like you anticipated.\n",
        "\n",
        "4. If you have time left after this, you can try with different politicians or other handles. For instance, @THL_org seems quite frequent - what is it and what do twitters tweet about it?"
      ],
      "metadata": {
        "id": "GdEOO_4JN8Oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tweets with @MarinSanna to a file\n",
        "#First check what you get with your command\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"@MarinSanna\" | head"
      ],
      "metadata": {
        "id": "JMMXw8bySgk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5265a4fa-918e-407b-9c96-c23fe28a51f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###C: NEWDOC\n",
            "###C:TIME 2020-03-30 23:59:55\n",
            "###C:TEXT @valtioneuvosto @MarinSanna Yllättävän hyvin hallitus on hoitanut asian ja pysynyt Ruotsin linjoilla. Siellä on perinteisesti tehty yhteiskunnan kannalta hyviä päätöksiä ja maa on menestynyt. #koronafi #koronasuomi #koronaepidemia\n",
            "\n",
            "\n",
            "1\t@valtioneuvosto @MarinSanna Yllättävän\tXvaltioneuvostosöminsannasyllättävä\tPROPN\tN\tCase=Nom|Number=Sing\t5\tadvmod\t_\t_\n",
            "2\thyvin\thyvin\tADV\tAdv\t_\t5\tadvmod\t_\t_\n",
            "3\thallitus\thallitus\tNOUN\tN\tCase=Nom|Number=Sing\t5\tnsubj\t_\t_\n",
            "4\ton\tolla\tAUX\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t5\taux\t_\t_\n",
            "5\thoitanut\thoitaa\tVERB\tV\tCase=Nom|Degree=Pos|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Act\t0\troot\t_\t_\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ATP_kurssi/data/ATP_kurssi/data/../scripts/read_conllu_docs.py\", line 74, in <module>\n",
            "    print_all(time,text,feats)\n",
            "  File \"/content/ATP_kurssi/data/ATP_kurssi/data/../scripts/read_conllu_docs.py\", line 57, in print_all\n",
            "    print(\"\\t\".join(f))\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Then direct to a file\n",
        "\n",
        "! zcat covidtweets.conllu.gz | python3 ../scripts/read_conllu_docs.py --text \"@MarinSanna\" | gzip > sannamarin.conllu.gz"
      ],
      "metadata": {
        "id": "XepV5s-kSRr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And check that the file looks as intended\n",
        "\n",
        "! zcat sannamarin.conllu.gz | head -50 # these look ok!"
      ],
      "metadata": {
        "id": "Cu2onssDTIA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97539bf-ae20-4add-fa9e-f89ccce649c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###C: NEWDOC\n",
            "###C:TIME 2020-03-30 23:59:55\n",
            "###C:TEXT @valtioneuvosto @MarinSanna Yllättävän hyvin hallitus on hoitanut asian ja pysynyt Ruotsin linjoilla. Siellä on perinteisesti tehty yhteiskunnan kannalta hyviä päätöksiä ja maa on menestynyt. #koronafi #koronasuomi #koronaepidemia\n",
            "\n",
            "\n",
            "1\t@valtioneuvosto @MarinSanna Yllättävän\tXvaltioneuvostosöminsannasyllättävä\tPROPN\tN\tCase=Nom|Number=Sing\t5\tadvmod\t_\t_\n",
            "2\thyvin\thyvin\tADV\tAdv\t_\t5\tadvmod\t_\t_\n",
            "3\thallitus\thallitus\tNOUN\tN\tCase=Nom|Number=Sing\t5\tnsubj\t_\t_\n",
            "4\ton\tolla\tAUX\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t5\taux\t_\t_\n",
            "5\thoitanut\thoitaa\tVERB\tV\tCase=Nom|Degree=Pos|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Act\t0\troot\t_\t_\n",
            "6\tasian\tasia\tNOUN\tN\tCase=Gen|Number=Sing\t5\tobj\t_\t_\n",
            "7\tja\tja\tCCONJ\tC\t_\t8\tcc\t_\t_\n",
            "8\tpysynyt\tpysyä\tVERB\tV\tCase=Nom|Degree=Pos|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Act\t5\tconj\t_\t_\n",
            "9\tRuotsin\tRuotsi\tPROPN\tN\tCase=Gen|Number=Sing\t10\tnmod:poss\t_\t_\n",
            "10\tlinjoilla\tlinja\tNOUN\tN\tCase=Ade|Number=Plur\t8\tobl\t_\t_\n",
            "11\t.\t.\tPUNCT\tPunct\t_\t5\tpunct\t_\t_\n",
            "\n",
            "\n",
            "1\tSiellä\tsiellä\tADV\tAdv\t_\t4\tadvmod\t_\t_\n",
            "2\ton\tolla\tAUX\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t4\taux:pass\t_\t_\n",
            "3\tperinteisesti\tperinteisesti\tADV\tAdv\tDerivation=Sti\t4\tadvmod\t_\t_\n",
            "4\ttehty\ttehdä\tVERB\tV\tCase=Nom|Degree=Pos|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Pass\t0\troot\t_\t_\n",
            "5\tyhteiskunnan\tyhteis#kunta\tNOUN\tN\tCase=Gen|Number=Sing\t6\tnmod:poss\t_\t_\n",
            "6\tkannalta\tkanta\tNOUN\tN\tCase=Abl|Number=Sing\t7\tnmod\t_\t_\n",
            "7\thyviä\thyvä\tADJ\tA\tCase=Par|Degree=Pos|Number=Plur\t8\tamod\t_\t_\n",
            "8\tpäätöksiä\tpäätös\tNOUN\tN\tCase=Par|Number=Plur\t4\tobj\t_\t_\n",
            "9\tja\tja\tCCONJ\tC\t_\t12\tcc\t_\t_\n",
            "10\tmaa\tmaa\tNOUN\tN\tCase=Nom|Number=Sing\t12\tnsubj\t_\t_\n",
            "11\ton\tolla\tAUX\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t12\taux\t_\t_\n",
            "12\tmenestynyt\tmenestyä\tVERB\tV\tCase=Nom|Degree=Pos|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Act\t4\tconj\t_\t_\n",
            "13\t.\t.\tPUNCT\tPunct\t_\t4\tpunct\t_\t_\n",
            "\n",
            "\n",
            "1\t#\t#\tPUNCT\tPunct\t_\t2\tpunct\t_\t_\n",
            "2\tkoronafi\tkoronafi\tNOUN\tN\tCase=Nom|Number=Sing\t0\troot\t_\t_\n",
            "3\t#\t#\tPUNCT\tPunct\t_\t4\tpunct\t_\t_\n",
            "4\tkoronasuomi\tkoron#asuomi\tNOUN\tN\tCase=Nom|Number=Sing\t2\tconj\t_\t_\n",
            "5\t#\t#\tPUNCT\tPunct\t_\t6\tpunct\t_\t_\n",
            "6\tkoronaepidemia\tkorona#epidemia\tNOUN\tN\tCase=Nom|Number=Sing\t2\tconj\t_\t_\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "###C: NEWDOC\n",
            "###C:TIME 2020-03-30 21:29:09\n",
            "###C:TEXT @valtioneuvosto @MarinSanna, onko nyt toimittu tuon ohjeen mukaan? Kansallinen varautumissuunnitelma influenssapandemiaa varten - STM julkaisuja 2012:9. ”...WHO on kehottanut jäsenmaitaan päivittämään  pandemian varautumissuunnitelmiaan...\" https://t.co/CuqP44dflB #korona\n",
            "\n",
            "\n",
            "1\t@valtioneuvosto @MarinSanna\t@valtioneuvosto @MarinSanna\tPROPN\tN\tCase=Nom|Number=Sing\t5\tvocative\t_\t_\n",
            "2\t,\t,\tPUNCT\tPunct\t_\t5\tpunct\t_\t_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.a) How many tweets does this file have?\n",
        "# by grepping and counting the lines starting with ###C:TEXT we can count the number of tweets\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TEXT\" | wc -l"
      ],
      "metadata": {
        "id": "yFVdGxjSTYea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931b1c9d-cb2e-4f90-e86a-c2829408add8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How are they distributed over time?\n",
        "# By grepping and counting the time stamps we can get their distribution over time\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" | sort | uniq -c | sort -rn | head\n",
        "\n",
        "# but clearly these stamps are too detailed, no trends can be seen"
      ],
      "metadata": {
        "id": "CHyCLR1OTvEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71dbad72-1b00-4c86-9e91-fc661351654a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      2 ###C:TIME 2020-04-11 10:58:51\n",
            "      2 ###C:TIME 2020-03-24 17:02:43\n",
            "      2 ###C:TIME 2020-03-24 07:07:42\n",
            "      2 ###C:TIME 2020-03-24 07:00:25\n",
            "      2 ###C:TIME 2020-03-24 06:56:25\n",
            "      2 ###C:TIME 2020-03-24 06:24:37\n",
            "      2 ###C:TIME 2020-03-24 05:05:22\n",
            "      2 ###C:TIME 2020-03-24 03:22:33\n",
            "      2 ###C:TIME 2020-03-24 03:13:13\n",
            "      2 ###C:TIME 2020-03-24 02:57:34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.b) What would be a reasonable way to analyze the distribution?\n",
        "# I'll take months (and delete the days and times)\n",
        "\n",
        "# for the dates\n",
        "#cut -f 2 -d ' ' (second column, delimiter white space)\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" |  cut -f 2 -d ' '| head"
      ],
      "metadata": {
        "id": "g7Wno0mlUguX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c55507-0dd1-4580-d698-7227c96bf9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020-03-30\n",
            "2020-03-30\n",
            "2020-03-30\n",
            "2020-03-30\n",
            "2020-03-30\n",
            "2020-03-30\n",
            "2020-03-30\n",
            "2020-03-30\n",
            "2020-03-30\n",
            "2020-03-30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for the months\n",
        "\n",
        "#cut -f 2 -d ' ' | cut -f 1,2 -d '-'\n",
        "# first take the dates, then from the dates, take columns 1 and 2 (year and month) delimited by a hyphen\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" |  cut -f 2 -d ' '| cut -f 1,2 -d '-' | head # this looks ok!"
      ],
      "metadata": {
        "id": "yvCE-KNfVETr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b388bc-6e19-45f9-969f-04914cf206a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2020-03\n",
            "2020-03\n",
            "2020-03\n",
            "2020-03\n",
            "2020-03\n",
            "2020-03\n",
            "2020-03\n",
            "2020-03\n",
            "2020-03\n",
            "2020-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to make sure, let's see how the distribution is\n",
        "#i.e. make a frequency list\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"^###C:TI\" |  cut -f 2 -d ' '| cut -f 1,2 -d '-' | sort | uniq -c | sort -rn | head -30\n",
        "\n",
        "# or, actually, I think the distribution is kinda wonky, mostly just from the first months of the crisis."
      ],
      "metadata": {
        "id": "aPyQt-vbVNSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045a2582-514f-422b-81cc-52a8d3a7c142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   1398 2020-03\n",
            "    850 2020-04\n",
            "      9 2019-12\n",
            "      7 2020-02\n",
            "      4 2020-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's try to compare the contents of tweets mentioning @MarinSanna at different periods of time.\n",
        "\n",
        "# what about the first days of the crisis? What would be frequent enough?\n",
        "# Based on this frequency list, I'll take March 17 and March 18\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"TIME\" | egrep \"2020-03\" | cut -f 2 -d ' ' | cut -f 1,2,3 -d '-'| sort | uniq -c | sort -rn | head -30"
      ],
      "metadata": {
        "id": "TwrG2N-BjanK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002d4ff8-44d1-4cbb-874f-3e5dc96fd152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    184 2020-03-21\n",
            "    176 2020-03-22\n",
            "    111 2020-03-25\n",
            "    107 2020-03-23\n",
            "     97 2020-03-18\n",
            "     96 2020-03-20\n",
            "     91 2020-03-24\n",
            "     84 2020-03-30\n",
            "     82 2020-03-19\n",
            "     79 2020-03-17\n",
            "     77 2020-03-29\n",
            "     75 2020-03-26\n",
            "     64 2020-03-28\n",
            "     62 2020-03-27\n",
            "      7 2020-03-14\n",
            "      3 2020-03-11\n",
            "      1 2020-03-16\n",
            "      1 2020-03-15\n",
            "      1 2020-03-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unfortunately my python script doesn't support regexes\n",
        "#let's gather the tweets from the chosen dates to two files\n",
        "\n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-03-18\" > 03-18.conllu\n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-03-17\" > 03-17.conllu"
      ],
      "metadata": {
        "id": "bTEHvDd7lDxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to check the numbers match - looks like ok!\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu | egrep \"###C: NEWDOC\" | wc -l\n",
        "! cat 03-18.conllu 03-17.conllu | egrep \"###C:TIME\" | cut -f 2 -d ' ' | cut -f 1,2,3 -d '-' | sort | uniq -c | sort -rn"
      ],
      "metadata": {
        "id": "jx_cpDpplfn_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08d4805-490f-44b0-b601-9969ee4c7c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176\n",
            "     97 2020-03-18\n",
            "     79 2020-03-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.a) Analyze possible differences in terms of frequent words.\n",
        "#Which POS classes provide the most interpretable results?\n",
        "\n",
        "# let's check the most frequent adjectives, nouns, verbs\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu  | egrep \"ADJ|NOUN|VERB\" | cut -f 3 | sort | uniq -c | sort -rn | head -20\n",
        "\n",
        "# I guess these are ok, but I'm not sure about the verbs. would the list be better without verbs?"
      ],
      "metadata": {
        "id": "vclBja4imt6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11bbe470-da6a-41ac-b27e-09ea14df41ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     60 hallitus\n",
            "     49 koron#avirus\n",
            "     43 koro\n",
            "     32 koronafi\n",
            "     29 pää#ministeri\n",
            "     24 valmius#laki\n",
            "     17 olla\n",
            "     16 tehdä\n",
            "     15 saada\n",
            "     14 kiitos\n",
            "     12 tulla\n",
            "     12 ihminen\n",
            "     12 hyvä\n",
            "     11 pitää\n",
            "     11 ministeri\n",
            "     10 tiedotus#tilaisuus\n",
            "     10 aika\n",
            "      9 työ\n",
            "      9 toimi\n",
            "      9 tilanne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's leave the verbs out\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu  | egrep \"ADJ|NOUN\" | cut -f 3 | sort | uniq -c | sort -rn | head -20\n"
      ],
      "metadata": {
        "id": "wQJ4BfClocjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e99dbbd-1a27-460b-99c3-ff9feb097efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     60 hallitus\n",
            "     49 koron#avirus\n",
            "     43 koro\n",
            "     32 koronafi\n",
            "     29 pää#ministeri\n",
            "     24 valmius#laki\n",
            "     14 kiitos\n",
            "     12 ihminen\n",
            "     12 hyvä\n",
            "     11 ministeri\n",
            "     10 tiedotus#tilaisuus\n",
            "     10 aika\n",
            "      9 työ\n",
            "      9 toimi\n",
            "      9 tilanne\n",
            "      9 suomalainen\n",
            "      9 presidentti\n",
            "      9 klo\n",
            "      8 tärkeä\n",
            "      8 raja\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# well, I guess these do reflect some moments in the crisis!\n",
        "# there's valmiuslaki \"emergency powers legislation\", tärkeä \"important\", raja \"border\"\n",
        "# I'll yet check how these are used in the tweets. Unf. my script doesn't search for lemmas, but maybe the string will get some matches anyway!\n",
        "\n",
        "#to get the texts only, egrep \"###C:TEXT\" after the option\n",
        "\n",
        "#N.B. two files can be read simultaneously by listing them after cat\n",
        "\n",
        "! cat 03-18.conllu 03-17.conllu  | python3 ../scripts/read_conllu_docs.py --text \"raja\" | egrep \"###C:TEXT\" | head\n",
        "\n",
        "# well, not horrible! The tweets mostly seem to discuss borders and border control"
      ],
      "metadata": {
        "id": "yMcGivZFohZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98a07bd-60f5-4f94-b144-e2851d4feaf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###C:TEXT Lähimaksun raja nostettava 100 euroon: vähennetään kosketustartunnan mahdollisuutta! @MarinSanna @DanskeBankFI @Nordea_FI @Osuuspankki @S_Pankki #koronafi\n",
            "###C:TEXT Hallitus: Lääkkeiden myyntiä voidaan nyt rajoittaa. Suomen rajat sulkeutuvat torstaina. #politiikka #korona #koronavirus #poikkeuslaki #hallitus @MarinSanna  https://t.co/Ktn7jTsuOE\n",
            "###C:TEXT Ei tämä nainen aio vartioida rajaa vaan tekee mitä itse haluaa. Sanna Marin tyytyy yhä hallituksen tiedottajan rooliin, kun kaivattaisiin jo pikkuhiljaa Pääministeriä.@MarinSanna #hallitus #korona @Demarit @AnttiRinnepj @rajavartijat https://t.co/NWHCHcCaGM\n",
            "###C:TEXT @nameKimmo @MarinSanna Nojaa, rajat ovat kiinni suomalaisilta mutta matut saavat tulla Suomeen ihan niinkuin ennenkin ja Suomesta poistumistakaan tuskin estetään. #rajatkiinni suomalaisilta, läpsyt läpsyttelevät rajan yli miten haluavat. #Koronavirusfi #politiikka\n",
            "###C:TEXT @valtioneuvosto @MariaOhisalo @MarinSanna @Haavisto @TimoHarakka @akpekonen @SebastianTyne @mauripeltokang2 @PetteriOrpo Rajat auki turvapaikanhakijoille. Rajat ovat siis auki. #Koronavirus liikkuu myös heidän mukana. Oppositiota kustiin silmään.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the comparison to April. Again, we need enough tweets, preferably from late April so there's some time gap to the March tweets we have. Let's see how many tweets we have from late April"
      ],
      "metadata": {
        "id": "kMHZAl_9pkoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.b) Can you see some differences that could be related to the different periods in the crisis and events that took place?\n",
        "# these are now just the days of April\n",
        "\n",
        "#cut -f 2 -d ' ' | egrep \"2020-04\" | cut -f 3 -d '-'\n",
        "#take the year-month-day column, then grep April in 2020, and finally take the day column separated by a hyphen\n",
        "\n",
        "! zcat sannamarin.conllu.gz | egrep \"###C:TIME\" | cut -f 2 -d ' ' | egrep \"2020-04\" | cut -f 3 -d '-' | sort | uniq -c | sort -rn\n",
        "\n",
        "# not many tweets from late April, but let's take April 15 and 14"
      ],
      "metadata": {
        "id": "kumNSsIDr6ZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c761460b-19ab-4fec-b9e3-ebd96e31746d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    118 08\n",
            "     77 02\n",
            "     74 11\n",
            "     60 10\n",
            "     59 09\n",
            "     51 15\n",
            "     50 14\n",
            "     49 04\n",
            "     49 03\n",
            "     48 07\n",
            "     46 01\n",
            "     43 16\n",
            "     39 05\n",
            "     31 06\n",
            "     30 13\n",
            "     26 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#direct the tweets from the chosen dates to files\n",
        "\n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-04-15\" >  04-15.conllu\n",
        "! zcat sannamarin.conllu.gz | python3 ../scripts/read_conllu_docs.py --time \"2020-04-14\" >  04-14.conllu"
      ],
      "metadata": {
        "id": "nMI52epLXygc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the same POS for April as for March\n",
        "\n",
        "#N.B. cat 04* matches all files that start with 04, so in this case both April files\n",
        "\n",
        "! cat 04*  | egrep \"ADJ|NOUN\" | cut -f 3 | sort | uniq -c | sort -rn | head -20"
      ],
      "metadata": {
        "id": "50vi0hlolcez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b61b94-3b72-4910-f806-edd1f1d19add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     23 hallitus\n",
            "     16 mökki\n",
            "     15 koronafi\n",
            "     13 koro\n",
            "     12 hyvä\n",
            "     10 maski\n",
            "     10 koron#akriisi\n",
            "      9 uutinen\n",
            "      9 apu\n",
            "      9 aika\n",
            "      8 raja\n",
            "      8 koron#avirus\n",
            "      7 övaltio#neuvosto\n",
            "      7 kansa\n",
            "      6 suuri\n",
            "      6 päivä\n",
            "      6 maa\n",
            "      5 virus\n",
            "      5 suomalainen\n",
            "      5 pää#ministeri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I guess there's some differences, at least mökki \"summer house\" and maski \"mask\". Let's see how these tweets with these words look like."
      ],
      "metadata": {
        "id": "L-KkpLE1uOUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the words in the tweets\n",
        "\n",
        "! cat 04* | python3 ../scripts/read_conllu_docs.py --text \"mökki\" | egrep \"###C:TEXT\" | head\n",
        "\n",
        "# well, looks like I should I'd think!"
      ],
      "metadata": {
        "id": "V2z2gN6_umfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef22609-076e-4b5e-fa70-dca52f9640e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###C:TEXT @hsfi Pitkään #hengityssuojain nähtiin hyödyttömäksi ja jopa vahingolliseksi.🤔 Täyskäännös seuraa pian myös näkemykselle mökkeilyn suhteen. Pian tullee todettavaksi, että mökki on paras #turvapaikka #korona -tartunnan välttämisessä!🌹Tutkijat töihin! @MarinSanna @YleAstudio @niinisto\n",
            "###C:TEXT Pitkään #hengityssuojain nähtiin hyödyttömäksi ja jopa vahingolliseksi.🤔 Täyskäännös seuraa pian myös näkemykselle mökkeilyn suhteen. Pian tullee todettavaksi, että mökki on paras #turvapaikka #korona -tartunnan välttämisessä!🌹Tutkijat töihin! @MarinSanna @YleAstudio @niinisto\n",
            "###C:TEXT Pitkään #hengityssuojain nähtiin hyödyttömäksi ja jopa vahingolliseksi.🤔 Täyskäännös seuraa pian myös näkemykselle mökkeilyn suhteen.Tullee pian todettavaksi,että mökki on paras #turvapaikka #korona -tartunnan välttämisessä!🌹Tutkijat töihin! @MarinSanna @ArttuY @SoiliVuorenmaa\n",
            "###C:TEXT Uudenmaan rajat aukeavar. @MarinSanna kumminkin sanoo ettå mökille ei mentäisi. Mutta uskooko tuota ne joilla on mökki? Suurin osa ei tule uskomaan!  #koronafi #riski\n",
            "###C:TEXT Kiitos @MarinSanna 🇫🇮  #VihdinSheriffi tervehtii Sinua #mökki:ltä.  Yhteinen taistelumme #SalenNyrkki:ä vastaan jatkuu #hellittämättä, kun ansaittu lepo #UudenmaanMotti:n murtamistaistelujen jälkeen on ohi.  #YhtenäinenSuomi #DeepStateExposed  #SuuriPuhdistus\n",
            "###C:TEXT @AnttiLindtman Puretuilla rajoilla jo ruuhkaa. Uskotteko te poliitikot todella, että tämä jukuripäinen kansa tottelee esim. nyt mökkiaikana matkustussuosituksia? Ei hyvältä näytä, kun tätä päätöstä taudin leviämisestä tuonnempana tarkastellaan. #korona @MarinSanna\n",
            "###C:TEXT @RealPeterNyman @MarinSanna No tuskin suositellaan MIHINKÄÄN lähtöä, rajoista viis. Mökkireissu poikii lähes varmasti muitakin pysähdyksiä, ruokakaupassa, alkossa, rautakaupassa jne. Nyt on hyvä pysyä vielä kotona jos vaan mahdollista. Näin aion itse tehdä edelleen vaikka mökki houkuttelisi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of useful commands in these exercises**\n",
        "\n",
        "`egrep \"^1[[:space:]]\"`\n",
        "\n",
        "match **tab** in egrep\n",
        "\n",
        "`egrep \"ADJ|NOUN|VERB\"`\n",
        "\n",
        "grep lines with ADJ, NOUN, **or** VERB\n",
        "\n",
        "`cut -f 3,4`\n",
        "\n",
        "get **several columns**; N.B. no white space between the column numbers\n",
        "\n",
        "`cut -f 3 -d '-'`\n",
        "\n",
        "get a **column separated by** a hyphen; d in `-d ' '` stands for delimiter, while the delimiter comes in single quotation marks; the default delimiter is tab\n",
        "\n",
        "`cat 03-18.conllu 03-17.conllu`\n",
        "\n",
        "print **several files simulatenously**\n",
        "\n",
        "`cat 04*`\n",
        "\n",
        "print all files **starting** with 04\n",
        "\n",
        "`cat *.txt`\n",
        "\n",
        "print all files **ending** with .txt"
      ],
      "metadata": {
        "id": "aXOg3tt9afaU"
      }
    }
  ]
}